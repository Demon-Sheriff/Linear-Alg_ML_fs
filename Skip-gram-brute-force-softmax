{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[],"dockerImageVersionId":30918,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-04-10T10:42:03.165910Z","iopub.execute_input":"2025-04-10T10:42:03.166146Z","iopub.status.idle":"2025-04-10T10:42:03.170972Z","shell.execute_reply.started":"2025-04-10T10:42:03.166094Z","shell.execute_reply":"2025-04-10T10:42:03.169986Z"}},"outputs":[],"execution_count":39},{"cell_type":"code","source":"!pip install cupy-cuda12x","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-10T10:42:04.748782Z","iopub.execute_input":"2025-04-10T10:42:04.749089Z","iopub.status.idle":"2025-04-10T10:42:08.158529Z","shell.execute_reply.started":"2025-04-10T10:42:04.749064Z","shell.execute_reply":"2025-04-10T10:42:08.157626Z"}},"outputs":[{"name":"stdout","text":"Requirement already satisfied: cupy-cuda12x in /usr/local/lib/python3.10/dist-packages (12.2.0)\nRequirement already satisfied: numpy<1.27,>=1.20 in /usr/local/lib/python3.10/dist-packages (from cupy-cuda12x) (1.26.4)\nRequirement already satisfied: fastrlock>=0.5 in /usr/local/lib/python3.10/dist-packages (from cupy-cuda12x) (0.8.2)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.10/dist-packages (from numpy<1.27,>=1.20->cupy-cuda12x) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.10/dist-packages (from numpy<1.27,>=1.20->cupy-cuda12x) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.10/dist-packages (from numpy<1.27,>=1.20->cupy-cuda12x) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.10/dist-packages (from numpy<1.27,>=1.20->cupy-cuda12x) (2025.0.1)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.10/dist-packages (from numpy<1.27,>=1.20->cupy-cuda12x) (2022.0.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.10/dist-packages (from numpy<1.27,>=1.20->cupy-cuda12x) (2.4.1)\nRequirement already satisfied: intel-openmp>=2024 in /usr/local/lib/python3.10/dist-packages (from mkl->numpy<1.27,>=1.20->cupy-cuda12x) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.10/dist-packages (from mkl->numpy<1.27,>=1.20->cupy-cuda12x) (2022.0.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.10/dist-packages (from tbb==2022.*->mkl->numpy<1.27,>=1.20->cupy-cuda12x) (1.2.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.10/dist-packages (from mkl_umath->numpy<1.27,>=1.20->cupy-cuda12x) (2024.2.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.10/dist-packages (from intel-openmp>=2024->mkl->numpy<1.27,>=1.20->cupy-cuda12x) (2024.2.0)\n","output_type":"stream"}],"execution_count":40},{"cell_type":"code","source":"!nvcc --version","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-10T10:42:09.354507Z","iopub.execute_input":"2025-04-10T10:42:09.354837Z","iopub.status.idle":"2025-04-10T10:42:09.520143Z","shell.execute_reply.started":"2025-04-10T10:42:09.354802Z","shell.execute_reply":"2025-04-10T10:42:09.519228Z"}},"outputs":[{"name":"stdout","text":"nvcc: NVIDIA (R) Cuda compiler driver\nCopyright (c) 2005-2023 NVIDIA Corporation\nBuilt on Tue_Aug_15_22:02:13_PDT_2023\nCuda compilation tools, release 12.2, V12.2.140\nBuild cuda_12.2.r12.2/compiler.33191640_0\n","output_type":"stream"}],"execution_count":41},{"cell_type":"code","source":"import cupy as cp","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-10T10:42:10.491675Z","iopub.execute_input":"2025-04-10T10:42:10.491989Z","iopub.status.idle":"2025-04-10T10:42:10.496087Z","shell.execute_reply.started":"2025-04-10T10:42:10.491964Z","shell.execute_reply":"2025-04-10T10:42:10.495193Z"}},"outputs":[],"execution_count":42},{"cell_type":"code","source":"import gensim.downloader as api\ndataset = api.load(\"text8\")  # or \"text8\", \"wiki-english-20171001\", etc.","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-10T10:42:11.922237Z","iopub.execute_input":"2025-04-10T10:42:11.922564Z","iopub.status.idle":"2025-04-10T10:42:12.117257Z","shell.execute_reply.started":"2025-04-10T10:42:11.922535Z","shell.execute_reply":"2025-04-10T10:42:12.116486Z"}},"outputs":[],"execution_count":43},{"cell_type":"code","source":"sentences = [list(sent) for sent in dataset]\nwords = 0\nfor s in sentences:\n    words += len(s)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-10T10:42:15.968492Z","iopub.execute_input":"2025-04-10T10:42:15.968824Z","iopub.status.idle":"2025-04-10T10:42:19.224471Z","shell.execute_reply.started":"2025-04-10T10:42:15.968798Z","shell.execute_reply":"2025-04-10T10:42:19.223539Z"}},"outputs":[],"execution_count":44},{"cell_type":"code","source":"words # 17005207","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-10T10:42:23.015379Z","iopub.execute_input":"2025-04-10T10:42:23.015665Z","iopub.status.idle":"2025-04-10T10:42:23.020916Z","shell.execute_reply.started":"2025-04-10T10:42:23.015644Z","shell.execute_reply":"2025-04-10T10:42:23.020046Z"}},"outputs":[{"execution_count":45,"output_type":"execute_result","data":{"text/plain":"17005207"},"metadata":{}}],"execution_count":45},{"cell_type":"code","source":"flat_words = [word for sentence in sentences for word in sentence]\nlen(flat_words)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-10T10:42:24.433275Z","iopub.execute_input":"2025-04-10T10:42:24.433582Z","iopub.status.idle":"2025-04-10T10:42:25.277652Z","shell.execute_reply.started":"2025-04-10T10:42:24.433558Z","shell.execute_reply":"2025-04-10T10:42:25.276862Z"}},"outputs":[{"execution_count":46,"output_type":"execute_result","data":{"text/plain":"17005207"},"metadata":{}}],"execution_count":46},{"cell_type":"code","source":"len(set(flat_words)) # unique words in the complete corpus -> vocab size","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-10T10:42:26.973957Z","iopub.execute_input":"2025-04-10T10:42:26.974377Z","iopub.status.idle":"2025-04-10T10:42:28.086390Z","shell.execute_reply.started":"2025-04-10T10:42:26.974343Z","shell.execute_reply":"2025-04-10T10:42:28.085417Z"}},"outputs":[{"execution_count":47,"output_type":"execute_result","data":{"text/plain":"253854"},"metadata":{}}],"execution_count":47},{"cell_type":"code","source":"c = 0\nfor w in (\"queen\", \"king\", \"man\", \"woman\"):\n    if w in set(flat_words[:100000]):\n        c += 1\n    else:\n        print(f\"{w} not found in corpus\")\nif c == 4:\n    print(\"all words present int the corpus\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-10T10:42:29.294796Z","iopub.execute_input":"2025-04-10T10:42:29.295086Z","iopub.status.idle":"2025-04-10T10:42:29.315174Z","shell.execute_reply.started":"2025-04-10T10:42:29.295064Z","shell.execute_reply":"2025-04-10T10:42:29.314274Z"}},"outputs":[{"name":"stdout","text":"all words present int the corpus\n","output_type":"stream"}],"execution_count":48},{"cell_type":"code","source":"corpus = flat_words[:100000]\nvocab = set(flat_words[:100000])\nvocab_size = len(vocab)\nm = 4 # context window of 9 words [4 future and 4 history and 1 center word]\nd = 100 # starting out with 100 dimensionality for word vectors","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-10T11:01:58.465665Z","iopub.execute_input":"2025-04-10T11:01:58.465986Z","iopub.status.idle":"2025-04-10T11:01:58.475305Z","shell.execute_reply.started":"2025-04-10T11:01:58.465960Z","shell.execute_reply":"2025-04-10T11:01:58.474215Z"}},"outputs":[],"execution_count":65},{"cell_type":"code","source":"len(vocab), len(corpus)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-10T11:01:59.517268Z","iopub.execute_input":"2025-04-10T11:01:59.517542Z","iopub.status.idle":"2025-04-10T11:01:59.522721Z","shell.execute_reply.started":"2025-04-10T11:01:59.517521Z","shell.execute_reply":"2025-04-10T11:01:59.521947Z"}},"outputs":[{"execution_count":66,"output_type":"execute_result","data":{"text/plain":"(12023, 100000)"},"metadata":{}}],"execution_count":66},{"cell_type":"code","source":"total_iterations = epochs * len(corpus) * (2 * m)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-10T11:02:00.205534Z","iopub.execute_input":"2025-04-10T11:02:00.205847Z","iopub.status.idle":"2025-04-10T11:02:00.209504Z","shell.execute_reply.started":"2025-04-10T11:02:00.205821Z","shell.execute_reply":"2025-04-10T11:02:00.208646Z"}},"outputs":[],"execution_count":67},{"cell_type":"code","source":"# Init embeddings\nU = cp.random.normal(0, 1.0/d, (vocab_size, d))  # Context\nV = cp.random.normal(0, 1.0/d, (vocab_size, d))  # Center","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-10T11:02:01.366468Z","iopub.execute_input":"2025-04-10T11:02:01.366765Z","iopub.status.idle":"2025-04-10T11:02:01.371303Z","shell.execute_reply.started":"2025-04-10T11:02:01.366742Z","shell.execute_reply":"2025-04-10T11:02:01.370394Z"}},"outputs":[],"execution_count":68},{"cell_type":"code","source":"from tqdm import tqdm","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-10T11:02:02.692060Z","iopub.execute_input":"2025-04-10T11:02:02.692457Z","iopub.status.idle":"2025-04-10T11:02:02.696299Z","shell.execute_reply.started":"2025-04-10T11:02:02.692429Z","shell.execute_reply":"2025-04-10T11:02:02.695392Z"}},"outputs":[],"execution_count":69},{"cell_type":"code","source":"epochs = 20 # starting with 20 epochs for now.\nlr = 0.05 # learning rate","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-10T11:02:03.952409Z","iopub.execute_input":"2025-04-10T11:02:03.952738Z","iopub.status.idle":"2025-04-10T11:02:03.956544Z","shell.execute_reply.started":"2025-04-10T11:02:03.952712Z","shell.execute_reply":"2025-04-10T11:02:03.955769Z"}},"outputs":[],"execution_count":70},{"cell_type":"code","source":"# Use the softmax trick\n# Apply gradient clipping\n# Apply parameter updates for best loss encountered\n# Apply early stopping\n# Applying subsampling\n# Applying learning rate decay","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-10T11:02:06.052540Z","iopub.execute_input":"2025-04-10T11:02:06.052853Z","iopub.status.idle":"2025-04-10T11:02:06.056366Z","shell.execute_reply.started":"2025-04-10T11:02:06.052829Z","shell.execute_reply":"2025-04-10T11:02:06.055459Z"}},"outputs":[],"execution_count":71},{"cell_type":"code","source":"loss_history = []\nbest_loss = float('inf')\nU_best = None\nV_best = None","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-10T11:02:07.387220Z","iopub.execute_input":"2025-04-10T11:02:07.387542Z","iopub.status.idle":"2025-04-10T11:02:07.391367Z","shell.execute_reply.started":"2025-04-10T11:02:07.387518Z","shell.execute_reply":"2025-04-10T11:02:07.390403Z"}},"outputs":[],"execution_count":72},{"cell_type":"code","source":"# prepare training pairs \nword_to_index = {word: idx for idx, word in enumerate(vocab)}\nindex_to_word = {idx: word for word, idx in word_to_index.items()}\n\ncorpus_ids = [word_to_index[word] for word in corpus]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-10T11:02:08.406695Z","iopub.execute_input":"2025-04-10T11:02:08.406974Z","iopub.status.idle":"2025-04-10T11:02:08.427194Z","shell.execute_reply.started":"2025-04-10T11:02:08.406954Z","shell.execute_reply":"2025-04-10T11:02:08.426314Z"}},"outputs":[],"execution_count":73},{"cell_type":"code","source":"with tqdm(total=total_iterations, desc=\"Training Skip-gram\") as pbar:\n    print(\"जय बजरंग बली\") # ॐ \n    for epoch in tqdm(range(epochs)):\n\n        epoch_loss = 0\n        # run through the corpus treating each word as a center word\n        for c_idx, center_word_id in enumerate(corpus_ids):\n            v_c = V[center_word_id, :]\n    \n            # iterate over the context window to find context|center (o|c) pairs\n            # context window would be from [c_idx - m, c_idx + m] excluding c_idx\n            # left boundary = max(0, c_idx - m), right boundary = min(corpus_size - 1, c_idx + m)\n            context_words = [idx for idx in range(max(0, c_idx - m), min(len(corpus_ids), c_idx + m + 1)) if idx != c_idx]\n            \n            for o_idx in context_words:\n                context_word_id = corpus_ids[o_idx]\n                # pair (o|c) -> (o_idx | c_idx)\n                u_o = U[context_word_id, :]\n                \n                # calculate softmax probability -> p(o|c)\n                dot_prods = cp.dot(U, v_c) # dot prodcuts of the entire context embedding wrods with the current center word\n                max_dot = cp.max(dot_prods) # max of the dot products for the softmax trick\n                smx_denom = cp.sum(cp.exp(dot_prods - max_dot)) # using the softmax trick\n                curr_dot = cp.dot(u_o, v_c) # dot product of the current center and context words\n                smx_num = cp.exp(curr_dot - max_dot)\n                p_o_c = smx_num/smx_denom \n    \n                # current loss of o and c is the log softmax prob\n                # log(smx_num) = curr_dot\n                curr_loss = -curr_dot + max_dot + cp.log(cp.sum(cp.exp(dot_prods - max_dot)))\n                epoch_loss += curr_loss\n    \n                # calculate p(w | c)\n                p_w_c = cp.exp(dot_prods - max_dot)/smx_denom\n                \n                # compute the gradients \n                # for v_c (center word) [calculated with the softmax trick]\n                grad_v_c = -u_o + cp.dot(p_w_c, U)\n    \n                # for u_w (other context words excluding the current context word)\n                # print(p_w_c.shape, v_c.shape)\n                grad_u_w = cp.outer(p_w_c, v_c)\n                grad_u_w[context_word_id, :] = 0\n    \n                # for u_o (current context word)\n                grad_u_o = v_c * (p_o_c - 1)\n\n                # update the parameters\n                V[center_word_id, :] -= lr * (grad_v_c)\n                U -= lr * (grad_u_w)\n                U[context_word_id, :] -= lr * (grad_u_o)\n            \n        avg_epoch_loss = epoch_loss / (len(corpus) * 2 * m)  # rough estimate of pairs\n        loss_history.append(avg_epoch_loss)\n        print(f\"Epoch {epoch + 1}, Loss: {avg_epoch_loss}\")\n\n        # save the best parameters\n        if avg_epoch_loss < best_loss:\n            U_best = U.copy()\n            V_best = V.copy()\n            best_loss = avg_epoch_loss\n                \npbar.close()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-10T11:02:10.358763Z","iopub.execute_input":"2025-04-10T11:02:10.359123Z"}},"outputs":[{"name":"stderr","text":"Training Skip-gram:   0%|          | 0/16000000 [00:00<?, ?it/s]","output_type":"stream"},{"name":"stdout","text":"जय बजरंग बली\n","output_type":"stream"},{"name":"stderr","text":"\n  0%|          | 0/20 [00:00<?, ?it/s]\u001b[A","output_type":"stream"},{"name":"stdout","text":"Epoch 0, Loss: 6.769050345657568\n","output_type":"stream"},{"name":"stderr","text":"\n  5%|▌         | 1/20 [10:07<3:12:18, 607.31s/it]\u001b[A\n 10%|█         | 2/20 [20:10<3:01:27, 604.88s/it]\u001b[A","output_type":"stream"},{"name":"stdout","text":"Epoch 1, Loss: 6.297657898846818\n","output_type":"stream"},{"name":"stderr","text":"\n 15%|█▌        | 3/20 [30:17<2:51:41, 606.00s/it]\u001b[A","output_type":"stream"},{"name":"stdout","text":"Epoch 2, Loss: 6.0868784080538605\n","output_type":"stream"},{"name":"stderr","text":"\n 20%|██        | 4/20 [40:21<2:41:18, 604.93s/it]\u001b[A","output_type":"stream"},{"name":"stdout","text":"Epoch 3, Loss: 5.9034091427176\n","output_type":"stream"},{"name":"stderr","text":"\n 25%|██▌       | 5/20 [50:24<2:31:06, 604.40s/it]\u001b[A","output_type":"stream"},{"name":"stdout","text":"Epoch 4, Loss: 5.7360555533027116\n","output_type":"stream"},{"name":"stderr","text":"\n 30%|███       | 6/20 [1:00:28<2:20:57, 604.14s/it]\u001b[A","output_type":"stream"},{"name":"stdout","text":"Epoch 5, Loss: 5.586203718928743\n","output_type":"stream"},{"name":"stderr","text":"\n 35%|███▌      | 7/20 [1:10:30<2:10:44, 603.39s/it]\u001b[A","output_type":"stream"},{"name":"stdout","text":"Epoch 6, Loss: 5.456922839869485\n","output_type":"stream"},{"name":"stderr","text":"\n 40%|████      | 8/20 [1:20:33<2:00:39, 603.33s/it]\u001b[A","output_type":"stream"},{"name":"stdout","text":"Epoch 7, Loss: 5.349607421434343\n","output_type":"stream"},{"name":"stderr","text":"\n 45%|████▌     | 9/20 [1:30:34<1:50:29, 602.71s/it]\u001b[A","output_type":"stream"},{"name":"stdout","text":"Epoch 8, Loss: 5.264393497579107\n","output_type":"stream"},{"name":"stderr","text":"\n 50%|█████     | 10/20 [1:40:36<1:40:24, 602.41s/it]\u001b[A","output_type":"stream"},{"name":"stdout","text":"Epoch 9, Loss: 5.199751967431902\n","output_type":"stream"},{"name":"stderr","text":"\n 55%|█████▌    | 11/20 [1:50:40<1:30:26, 602.99s/it]\u001b[A","output_type":"stream"},{"name":"stdout","text":"Epoch 10, Loss: 5.1523240995410164\n","output_type":"stream"},{"name":"stderr","text":"\n 60%|██████    | 12/20 [2:00:44<1:20:26, 603.31s/it]\u001b[A","output_type":"stream"},{"name":"stdout","text":"Epoch 11, Loss: 5.11806300842657\n","output_type":"stream"},{"name":"stderr","text":"\n 65%|██████▌   | 13/20 [2:10:47<1:10:21, 603.13s/it]\u001b[A","output_type":"stream"},{"name":"stdout","text":"Epoch 12, Loss: 5.093342040938001\n","output_type":"stream"}],"execution_count":null},{"cell_type":"code","source":"# # training using skip-gram\n\n# with tqdm(total=total_iterations, desc=\"Training Skip-gram\") as pbar:\n#     for epoch in tqdm(range(epochs)):\n        \n#         epoch_loss = 0.0\n#         num_pairs = 0\n#         # pick a center word\n#         for (idx, c) in (enumerate(vocab)):\n#             # for the context window (valid boundaries) [idx - m, idx + m]\n#             left_boundary = max(0, idx - m)\n#             right_boundary = min(len(vocab) - 1, idx + m)\n        \n#             # all the context word indices\n#             context_indices = [i for i in range(left_boundary, right_boundary + 1) if i != idx] # don't take the center word\n        \n#             # for each context (outside) word in the window \n#             for o_idx in context_indices:\n        \n#                 u_o = U[o_idx, :] # current context word embedding\n#                 v_c = V[idx, :] # centre word embedding\n                \n#                 # calculate p(w | c) softmax summation\n#                 dot_products = cp.dot(U, v_c)\n#                 log_sum_exp = cp.log(cp.sum(cp.exp(dot_products)))  # Log-sum-exp for stability\n#                 current_loss = -dot_products[o_idx] + log_sum_exp\n#                 epoch_loss += current_loss\n#                 num_pairs += 1\n                \n#                 exp_dots = cp.exp(dot_products) # TODO : use numerical stability \n#                 p_w_c = exp_dots / cp.sum(exp_dots) # P(w | c) for each context word [don't use w = c]\n        \n#                 # calculate gradients\n#                 vc_grad = -u_o + cp.dot(p_w_c, U) # gradient for center word\n#                 uo_grad = (p_w_c[o_idx] - 1)*v_c # gradient for current context word\n#                 # TODO -> understand why outer is being used and use negative sampling later.\n#                 uw_grad = cp.outer(p_w_c, v_c) # gradient for unobserved (leaving the current context word) context words\n#                 # adjust gradient for observed word o\n#                 uw_grad[o_idx, :] -= v_c  # Equivalent to (p(o|c) - 1) * v_c\n                \n#                 # Update the parameter vectors using the gradients\n#                 V[idx, : ] -= lr * (vc_grad) # update the center word\n#                 U[o_idx, : ] -= lr * (uo_grad) # update the current (observed) context word\n#                 U -= lr * (uw_grad) # update the unobserved context words\n                \n                \n#         avg_epoch_loss = float(epoch_loss / num_pairs)  # Convert to Python float\n#         loss_history.append(avg_epoch_loss)\n#         print(f\"Epoch {epoch}, Loss: {avg_epoch_loss}\")\n\n# pbar.close()","metadata":{"trusted":true,"execution":{"execution_failed":"2025-04-06T15:32:24.093Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# total_iterations = epochs * len(vocab)  # Approximate total pairs\n# with tqdm(total=total_iterations, desc=\"Training Skip-gram\") as pbar:\n#     for epoch in range(epochs):\n#         epoch_loss = 0.0\n#         num_pairs = 0\n#         for idx in range(len(vocab)):\n#             left_boundary = max(0, idx - m)\n#             right_boundary = min(len(vocab) - 1, idx + m)\n#             context_indices = [i for i in range(left_boundary, right_boundary + 1) if i != idx]\n#             for o_idx in context_indices:\n#                 u_o = U[o_idx, :]  # context embedding for o\n#                 v_c = V[idx, :]    # center embedding for c\n#                 dot_products = cp.dot(U, v_c)\n#                 log_sum_exp = cp.log(cp.sum(cp.exp(dot_products - cp.max(dot_products))) + cp.max(dot_products))\n#                 current_loss = -cp.dot(u_o, v_c) + log_sum_exp\n#                 epoch_loss += current_loss\n#                 num_pairs += 1\n                \n#                 exp_dots = cp.exp(dot_products - cp.max(dot_products))\n#                 p_w_c = exp_dots / cp.sum(exp_dots)\n                \n#                 vc_grad = -u_o + cp.dot(p_w_c, U)\n#                 uo_grad = (p_w_c[o_idx] - 1) * v_c\n#                 uw_grad = p_w_c * v_c\n#                 uw_grad[o_idx] = (p_w_c[o_idx] - 1) * v_c\n                \n#                 V[idx, :] -= lr * vc_grad\n#                 U[o_idx, :] -= lr * uo_grad\n#                 for w in range(len(vocab)):\n#                     U[w, :] -= lr * uw_grad[w]\n                \n#                 pbar.update(1)  # Update progress bar per pair\n        \n#         avg_epoch_loss = float(epoch_loss / num_pairs)\n#         loss_history.append(avg_epoch_loss)\n#         pbar.set_postfix({'Epoch Loss': avg_epoch_loss})\n#         print(f\"Epoch {epoch}, Loss: {avg_epoch_loss}\")\n\n# pbar.close()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-06T14:09:59.126062Z","iopub.execute_input":"2025-04-06T14:09:59.126394Z","iopub.status.idle":"2025-04-06T14:09:59.658349Z","shell.execute_reply.started":"2025-04-06T14:09:59.126365Z","shell.execute_reply":"2025-04-06T14:09:59.657104Z"}},"outputs":[{"name":"stderr","text":"Training Skip-gram:   0%|          | 0/1000000 [00:00<?, ?it/s]\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)","\u001b[0;32m<ipython-input-34-5deddc4135e1>\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     22\u001b[0m                 \u001b[0mvc_grad\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0mu_o\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mcp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mp_w_c\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mU\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m                 \u001b[0muo_grad\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mp_w_c\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mo_idx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mv_c\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m                 \u001b[0muw_grad\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mp_w_c\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mv_c\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     25\u001b[0m                 \u001b[0muw_grad\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mo_idx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mp_w_c\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mo_idx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mv_c\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32mcupy/_core/core.pyx\u001b[0m in \u001b[0;36mcupy._core.core._ndarray_base.__mul__\u001b[0;34m()\u001b[0m\n","\u001b[0;32mcupy/_core/_kernel.pyx\u001b[0m in \u001b[0;36mcupy._core._kernel.ufunc.__call__\u001b[0;34m()\u001b[0m\n","\u001b[0;32mcupy/_core/internal.pyx\u001b[0m in \u001b[0;36mcupy._core.internal._broadcast_core\u001b[0;34m()\u001b[0m\n","\u001b[0;31mValueError\u001b[0m: operands could not be broadcast together with shapes (10000,) (100,)"],"ename":"ValueError","evalue":"operands could not be broadcast together with shapes (10000,) (100,)","output_type":"error"}],"execution_count":34},{"cell_type":"code","source":"V","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-06T09:43:19.214963Z","iopub.execute_input":"2025-04-06T09:43:19.215316Z","iopub.status.idle":"2025-04-06T09:43:19.221847Z","shell.execute_reply.started":"2025-04-06T09:43:19.215290Z","shell.execute_reply":"2025-04-06T09:43:19.220814Z"}},"outputs":[{"execution_count":30,"output_type":"execute_result","data":{"text/plain":"array([[ 1.40136926, -0.49552651, -0.42532298],\n       [ 0.05313453, -0.03351528,  0.68138598],\n       [-0.23847167, -0.40992499, -0.06977735],\n       [-0.09327522,  0.22551045, -0.07777905],\n       [ 0.29441731, -0.07724471, -0.13224349],\n       [ 0.32116448,  0.18530075,  1.10928156],\n       [-0.60548336, -1.4123885 ,  0.1138442 ]])"},"metadata":{}}],"execution_count":30},{"cell_type":"code","source":"vocab","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-06T09:43:30.186898Z","iopub.execute_input":"2025-04-06T09:43:30.187256Z","iopub.status.idle":"2025-04-06T09:43:30.192765Z","shell.execute_reply.started":"2025-04-06T09:43:30.187229Z","shell.execute_reply":"2025-04-06T09:43:30.191977Z"}},"outputs":[{"execution_count":31,"output_type":"execute_result","data":{"text/plain":"['The', 'cat', 'and', 'dog', 'had', 'a', 'fight']"},"metadata":{}}],"execution_count":31},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}