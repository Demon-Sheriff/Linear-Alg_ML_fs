{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[],"dockerImageVersionId":30918,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-04-09T13:27:11.517354Z","iopub.execute_input":"2025-04-09T13:27:11.517735Z","iopub.status.idle":"2025-04-09T13:27:11.522295Z","shell.execute_reply.started":"2025-04-09T13:27:11.517706Z","shell.execute_reply":"2025-04-09T13:27:11.521481Z"}},"outputs":[],"execution_count":7},{"cell_type":"code","source":"!pip install cupy-cuda12x","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-09T13:27:12.542459Z","iopub.execute_input":"2025-04-09T13:27:12.542803Z","iopub.status.idle":"2025-04-09T13:27:15.854501Z","shell.execute_reply.started":"2025-04-09T13:27:12.542774Z","shell.execute_reply":"2025-04-09T13:27:15.853512Z"}},"outputs":[{"name":"stdout","text":"Requirement already satisfied: cupy-cuda12x in /usr/local/lib/python3.10/dist-packages (12.2.0)\nRequirement already satisfied: numpy<1.27,>=1.20 in /usr/local/lib/python3.10/dist-packages (from cupy-cuda12x) (1.26.4)\nRequirement already satisfied: fastrlock>=0.5 in /usr/local/lib/python3.10/dist-packages (from cupy-cuda12x) (0.8.2)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.10/dist-packages (from numpy<1.27,>=1.20->cupy-cuda12x) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.10/dist-packages (from numpy<1.27,>=1.20->cupy-cuda12x) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.10/dist-packages (from numpy<1.27,>=1.20->cupy-cuda12x) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.10/dist-packages (from numpy<1.27,>=1.20->cupy-cuda12x) (2025.0.1)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.10/dist-packages (from numpy<1.27,>=1.20->cupy-cuda12x) (2022.0.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.10/dist-packages (from numpy<1.27,>=1.20->cupy-cuda12x) (2.4.1)\nRequirement already satisfied: intel-openmp>=2024 in /usr/local/lib/python3.10/dist-packages (from mkl->numpy<1.27,>=1.20->cupy-cuda12x) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.10/dist-packages (from mkl->numpy<1.27,>=1.20->cupy-cuda12x) (2022.0.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.10/dist-packages (from tbb==2022.*->mkl->numpy<1.27,>=1.20->cupy-cuda12x) (1.2.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.10/dist-packages (from mkl_umath->numpy<1.27,>=1.20->cupy-cuda12x) (2024.2.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.10/dist-packages (from intel-openmp>=2024->mkl->numpy<1.27,>=1.20->cupy-cuda12x) (2024.2.0)\n","output_type":"stream"}],"execution_count":8},{"cell_type":"code","source":"!nvcc --version","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-09T13:27:15.855654Z","iopub.execute_input":"2025-04-09T13:27:15.855976Z","iopub.status.idle":"2025-04-09T13:27:15.985853Z","shell.execute_reply.started":"2025-04-09T13:27:15.855938Z","shell.execute_reply":"2025-04-09T13:27:15.985075Z"}},"outputs":[{"name":"stdout","text":"nvcc: NVIDIA (R) Cuda compiler driver\nCopyright (c) 2005-2023 NVIDIA Corporation\nBuilt on Tue_Aug_15_22:02:13_PDT_2023\nCuda compilation tools, release 12.2, V12.2.140\nBuild cuda_12.2.r12.2/compiler.33191640_0\n","output_type":"stream"}],"execution_count":9},{"cell_type":"code","source":"import cupy as cp","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-09T13:27:15.987181Z","iopub.execute_input":"2025-04-09T13:27:15.987420Z","iopub.status.idle":"2025-04-09T13:27:15.990963Z","shell.execute_reply.started":"2025-04-09T13:27:15.987396Z","shell.execute_reply":"2025-04-09T13:27:15.990199Z"}},"outputs":[],"execution_count":10},{"cell_type":"code","source":"import gensim.downloader as api\ndataset = api.load(\"text8\")  # or \"text8\", \"wiki-english-20171001\", etc.","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-09T13:27:17.735590Z","iopub.execute_input":"2025-04-09T13:27:17.735883Z","iopub.status.idle":"2025-04-09T13:27:17.772614Z","shell.execute_reply.started":"2025-04-09T13:27:17.735859Z","shell.execute_reply":"2025-04-09T13:27:17.772004Z"}},"outputs":[],"execution_count":11},{"cell_type":"code","source":"sentences = [list(sent) for sent in dataset]\nwords = 0\nfor s in sentences:\n    words += len(s)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-09T13:27:23.959664Z","iopub.execute_input":"2025-04-09T13:27:23.960006Z","iopub.status.idle":"2025-04-09T13:27:26.752537Z","shell.execute_reply.started":"2025-04-09T13:27:23.959976Z","shell.execute_reply":"2025-04-09T13:27:26.751840Z"}},"outputs":[],"execution_count":12},{"cell_type":"code","source":"words # 17005207","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-09T13:31:24.988569Z","iopub.execute_input":"2025-04-09T13:31:24.988872Z","iopub.status.idle":"2025-04-09T13:31:24.993904Z","shell.execute_reply.started":"2025-04-09T13:31:24.988850Z","shell.execute_reply":"2025-04-09T13:31:24.993057Z"}},"outputs":[{"execution_count":28,"output_type":"execute_result","data":{"text/plain":"17005207"},"metadata":{}}],"execution_count":28},{"cell_type":"code","source":"flat_words = [word for sentence in sentences for word in sentence]\nlen(flat_words)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-09T13:31:32.711770Z","iopub.execute_input":"2025-04-09T13:31:32.712057Z","iopub.status.idle":"2025-04-09T13:31:33.151198Z","shell.execute_reply.started":"2025-04-09T13:31:32.712035Z","shell.execute_reply":"2025-04-09T13:31:33.150423Z"}},"outputs":[{"execution_count":29,"output_type":"execute_result","data":{"text/plain":"17005207"},"metadata":{}}],"execution_count":29},{"cell_type":"code","source":"len(set(flat_words)) # unique words in the complete corpus -> vocab size","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-09T13:31:43.104970Z","iopub.execute_input":"2025-04-09T13:31:43.105318Z","iopub.status.idle":"2025-04-09T13:31:43.745082Z","shell.execute_reply.started":"2025-04-09T13:31:43.105288Z","shell.execute_reply":"2025-04-09T13:31:43.744401Z"}},"outputs":[{"execution_count":31,"output_type":"execute_result","data":{"text/plain":"253854"},"metadata":{}}],"execution_count":31},{"cell_type":"code","source":"c = 0\nfor w in (\"queen\", \"king\", \"man\", \"woman\"):\n    if w in set(flat_words[:100000]):\n        c += 1\n    else:\n        print(f\"{w} not found in corpus\")\nif c == 4:\n    print(\"all words present int the corpus\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-09T13:45:09.861098Z","iopub.execute_input":"2025-04-09T13:45:09.861422Z","iopub.status.idle":"2025-04-09T13:45:09.880480Z","shell.execute_reply.started":"2025-04-09T13:45:09.861359Z","shell.execute_reply":"2025-04-09T13:45:09.879590Z"}},"outputs":[{"name":"stdout","text":"all words present int the corpus\n","output_type":"stream"}],"execution_count":43},{"cell_type":"code","source":"corpus = flat_words[:100000]\nvocab = set(flat_words[:100000])\nvocab_size = len(vocab)\nm = 4 # context window of 9 words [4 future and 4 history and 1 center word]\nd = 100 # starting out with 100 dimensionality for word vectors","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-09T13:53:00.390887Z","iopub.execute_input":"2025-04-09T13:53:00.391218Z","iopub.status.idle":"2025-04-09T13:53:00.400238Z","shell.execute_reply.started":"2025-04-09T13:53:00.391193Z","shell.execute_reply":"2025-04-09T13:53:00.399415Z"}},"outputs":[],"execution_count":50},{"cell_type":"code","source":"len(vocab), len(corpus)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-09T13:53:01.083066Z","iopub.execute_input":"2025-04-09T13:53:01.083332Z","iopub.status.idle":"2025-04-09T13:53:01.088301Z","shell.execute_reply.started":"2025-04-09T13:53:01.083311Z","shell.execute_reply":"2025-04-09T13:53:01.087550Z"}},"outputs":[{"execution_count":51,"output_type":"execute_result","data":{"text/plain":"(12023, 100000)"},"metadata":{}}],"execution_count":51},{"cell_type":"code","source":"# Init embeddings\nU = cp.random.normal(0, 1.0/d, (vocab_size, d))  # Context\nV = cp.random.normal(0, 1.0/d, (vocab_size, d))  # Center","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-09T13:53:03.602674Z","iopub.execute_input":"2025-04-09T13:53:03.602978Z","iopub.status.idle":"2025-04-09T13:53:03.828044Z","shell.execute_reply.started":"2025-04-09T13:53:03.602955Z","shell.execute_reply":"2025-04-09T13:53:03.827090Z"}},"outputs":[],"execution_count":52},{"cell_type":"code","source":"U.shape, V.shape","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-09T13:53:15.041286Z","iopub.execute_input":"2025-04-09T13:53:15.041673Z","iopub.status.idle":"2025-04-09T13:53:15.047572Z","shell.execute_reply.started":"2025-04-09T13:53:15.041632Z","shell.execute_reply":"2025-04-09T13:53:15.046434Z"}},"outputs":[{"execution_count":53,"output_type":"execute_result","data":{"text/plain":"((12023, 100), (12023, 100))"},"metadata":{}}],"execution_count":53},{"cell_type":"code","source":"ctx_w = U[3 - 3 : 3 + 3 + 1, :]\nidx = 3\nc = ctx_w[idx, :]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-09T15:07:03.976159Z","iopub.execute_input":"2025-04-09T15:07:03.976452Z","iopub.status.idle":"2025-04-09T15:07:03.996938Z","shell.execute_reply.started":"2025-04-09T15:07:03.976421Z","shell.execute_reply":"2025-04-09T15:07:03.995761Z"}},"outputs":[],"execution_count":2},{"cell_type":"code","source":"k = cp.array([[1, 2, 3], [4, 5, 6], [7, 8, 9]])\nt = cp.array([1, 2, 1])\n(cp.dot(k, t))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-06T09:42:32.176807Z","iopub.execute_input":"2025-04-06T09:42:32.177147Z","iopub.status.idle":"2025-04-06T09:42:35.445708Z","shell.execute_reply.started":"2025-04-06T09:42:32.177122Z","shell.execute_reply":"2025-04-06T09:42:35.444858Z"}},"outputs":[{"execution_count":18,"output_type":"execute_result","data":{"text/plain":"array([ 8, 20, 32])"},"metadata":{}}],"execution_count":18},{"cell_type":"code","source":"(cp.exp(cp.dot(U, c))) / cp.sum(cp.exp(cp.dot(U, c)))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-06T09:42:41.076563Z","iopub.execute_input":"2025-04-06T09:42:41.076890Z","iopub.status.idle":"2025-04-06T09:42:41.346930Z","shell.execute_reply.started":"2025-04-06T09:42:41.076864Z","shell.execute_reply":"2025-04-06T09:42:41.346128Z"}},"outputs":[{"execution_count":23,"output_type":"execute_result","data":{"text/plain":"array([0.14287235, 0.14283062, 0.14289339, 0.14290207, 0.1428402 ,\n       0.14282173, 0.14283964])"},"metadata":{}}],"execution_count":23},{"cell_type":"code","source":"sf_sum = (cp.exp(cp.dot(ctx_w, c))).sum()\n((cp.dot(ctx_w, c)/sf_sum)[:3]).sum() + ((cp.dot(ctx_w, c)/sf_sum)[4:]).sum()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-06T09:42:42.605303Z","iopub.execute_input":"2025-04-06T09:42:42.605623Z","iopub.status.idle":"2025-04-06T09:42:42.800993Z","shell.execute_reply.started":"2025-04-06T09:42:42.605597Z","shell.execute_reply":"2025-04-06T09:42:42.800113Z"}},"outputs":[{"execution_count":24,"output_type":"execute_result","data":{"text/plain":"array(-4.6431794e-05)"},"metadata":{}}],"execution_count":24},{"cell_type":"code","source":"epochs = 100 # starting with 100 epochs for now.\nlr = 0.05 # learning rate","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-06T13:50:02.319282Z","iopub.execute_input":"2025-04-06T13:50:02.319564Z","iopub.status.idle":"2025-04-06T13:50:02.323029Z","shell.execute_reply.started":"2025-04-06T13:50:02.319540Z","shell.execute_reply":"2025-04-06T13:50:02.322167Z"}},"outputs":[],"execution_count":24},{"cell_type":"code","source":"loss_history = []","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-06T13:59:09.400611Z","iopub.execute_input":"2025-04-06T13:59:09.400906Z","iopub.status.idle":"2025-04-06T13:59:09.404780Z","shell.execute_reply.started":"2025-04-06T13:59:09.400884Z","shell.execute_reply":"2025-04-06T13:59:09.403769Z"}},"outputs":[],"execution_count":27},{"cell_type":"code","source":"from tqdm import tqdm","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-06T14:07:37.057035Z","iopub.execute_input":"2025-04-06T14:07:37.057348Z","iopub.status.idle":"2025-04-06T14:07:37.060942Z","shell.execute_reply.started":"2025-04-06T14:07:37.057326Z","shell.execute_reply":"2025-04-06T14:07:37.059954Z"}},"outputs":[],"execution_count":32},{"cell_type":"code","source":"# Use the softmax trick\n# Apply gradient clipping\n# Apply parameter updates for best loss encountered\n# Apply early stopping\n# Applying subsampling\n# Applying learning rate decay","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-09T13:56:26.624731Z","iopub.execute_input":"2025-04-09T13:56:26.625087Z","iopub.status.idle":"2025-04-09T13:56:26.628904Z","shell.execute_reply.started":"2025-04-09T13:56:26.625061Z","shell.execute_reply":"2025-04-09T13:56:26.627915Z"}},"outputs":[],"execution_count":60},{"cell_type":"code","source":"for epoch in range(epochs):\n\n    # run through the corpus treating each word as a center word\n    for c_idx, c in enumerate(corpus):\n        v_c = V[c_idx, :]\n\n        # iterate over the context window to find context|center (o|c) pairs\n        # context window would be from [c_idx - m, c_idx + m] excluding c_idx\n        # left boundary = max(0, c_idx - m), right boundary = min(corpus_size - 1, c_idx + m)\n        context_words = [idx for idx in range(max(0, c_idx - m), min(len(corpus) - 1, c_idx + m)) if idx != c_idx]\n\n        for o_idx in context_words:\n\n            # pair (o|c) -> (o_idx | c_idx)\n            u_o = U[o_idx, :]\n            \n            # calculate softmax probability -> p(o|c)\n            dot_prods = cp.dot(U, v_c) # dot prodcuts of the entire context embedding wrods with the current center word\n            smx_denom = cp.sum(cp.exp(dot_prods - cp.max(dot_prods))) + cp.max(dot_prods) # using the softmax trick\n            curr_dot = cp.dot(u_o, v_c) # dot product of the current center and context words\n            smx_num = cp.exp(curr_dot - cp.max(dot_prods))\n            p_o_c = smx_num/smx_denom \n\n            # current loss of o and c is the log softmax prob\n            # log(smx_num) = curr_dot\n            curr_loss = -curr_dot + cp.log(cp.sum(cp.exp(dot_prods)))\n\n            # calculate p(w | c)\n            p_w_c = cp.exp(dot_prods - cp.max(dot_prods)))/smx_denom\n            \n            # compute the gradients \n            # for v_c (center word) [calculated with the softmax trick]\n            grad_v_c = -u_o + cp.dot(p_w_c, U)\n\n            # for u_w (other context words excluding the current context word)\n            grad_u_w = (p_w_c * v_c)\n            grad_u_w[o_idx, :] = 0\n\n            # for u_o (current context word)\n            grad_u_o = v_c(p_o_c - 1)\n\n            # update the parameters\n            V[c_idx, :] -= lr * (grad_v_c)\n            U -= lr * (grad_u_w)\n            U[o_idx, :] -= lr * (grad_u_o)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-09T15:07:03.970410Z","iopub.execute_input":"2025-04-09T15:07:03.970677Z","iopub.status.idle":"2025-04-09T15:07:03.975167Z","shell.execute_reply.started":"2025-04-09T15:07:03.970653Z","shell.execute_reply":"2025-04-09T15:07:03.974049Z"}},"outputs":[],"execution_count":1},{"cell_type":"code","source":"# training using skip-gram\n\nwith tqdm(total=total_iterations, desc=\"Training Skip-gram\") as pbar:\n    for epoch in tqdm(range(epochs)):\n        \n        epoch_loss = 0.0\n        num_pairs = 0\n        # pick a center word\n        for (idx, c) in (enumerate(vocab)):\n            # for the context window (valid boundaries) [idx - m, idx + m]\n            left_boundary = max(0, idx - m)\n            right_boundary = min(len(vocab) - 1, idx + m)\n        \n            # all the context word indices\n            context_indices = [i for i in range(left_boundary, right_boundary + 1) if i != idx] # don't take the center word\n        \n            # for each context (outside) word in the window \n            for o_idx in context_indices:\n        \n                u_o = U[o_idx, :] # current context word embedding\n                v_c = V[idx, :] # centre word embedding\n                \n                # calculate p(w | c) softmax summation\n                dot_products = cp.dot(U, v_c)\n                log_sum_exp = cp.log(cp.sum(cp.exp(dot_products)))  # Log-sum-exp for stability\n                current_loss = -dot_products[o_idx] + log_sum_exp\n                epoch_loss += current_loss\n                num_pairs += 1\n                \n                exp_dots = cp.exp(dot_products) # TODO : use numerical stability \n                p_w_c = exp_dots / cp.sum(exp_dots) # P(w | c) for each context word [don't use w = c]\n        \n                # calculate gradients\n                vc_grad = -u_o + cp.dot(p_w_c, U) # gradient for center word\n                uo_grad = (p_w_c[o_idx] - 1)*v_c # gradient for current context word\n                # TODO -> understand why outer is being used and use negative sampling later.\n                uw_grad = cp.outer(p_w_c, v_c) # gradient for unobserved (leaving the current context word) context words\n                # adjust gradient for observed word o\n                uw_grad[o_idx, :] -= v_c  # Equivalent to (p(o|c) - 1) * v_c\n                \n                # Update the parameter vectors using the gradients\n                V[idx, : ] -= lr * (vc_grad) # update the center word\n                U[o_idx, : ] -= lr * (uo_grad) # update the current (observed) context word\n                U -= lr * (uw_grad) # update the unobserved context words\n                \n                \n        avg_epoch_loss = float(epoch_loss / num_pairs)  # Convert to Python float\n        loss_history.append(avg_epoch_loss)\n        print(f\"Epoch {epoch}, Loss: {avg_epoch_loss}\")\n\npbar.close()","metadata":{"trusted":true,"execution":{"execution_failed":"2025-04-06T15:32:24.093Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# total_iterations = epochs * len(vocab)  # Approximate total pairs\n# with tqdm(total=total_iterations, desc=\"Training Skip-gram\") as pbar:\n#     for epoch in range(epochs):\n#         epoch_loss = 0.0\n#         num_pairs = 0\n#         for idx in range(len(vocab)):\n#             left_boundary = max(0, idx - m)\n#             right_boundary = min(len(vocab) - 1, idx + m)\n#             context_indices = [i for i in range(left_boundary, right_boundary + 1) if i != idx]\n#             for o_idx in context_indices:\n#                 u_o = U[o_idx, :]  # context embedding for o\n#                 v_c = V[idx, :]    # center embedding for c\n#                 dot_products = cp.dot(U, v_c)\n#                 log_sum_exp = cp.log(cp.sum(cp.exp(dot_products - cp.max(dot_products))) + cp.max(dot_products))\n#                 current_loss = -cp.dot(u_o, v_c) + log_sum_exp\n#                 epoch_loss += current_loss\n#                 num_pairs += 1\n                \n#                 exp_dots = cp.exp(dot_products - cp.max(dot_products))\n#                 p_w_c = exp_dots / cp.sum(exp_dots)\n                \n#                 vc_grad = -u_o + cp.dot(p_w_c, U)\n#                 uo_grad = (p_w_c[o_idx] - 1) * v_c\n#                 uw_grad = p_w_c * v_c\n#                 uw_grad[o_idx] = (p_w_c[o_idx] - 1) * v_c\n                \n#                 V[idx, :] -= lr * vc_grad\n#                 U[o_idx, :] -= lr * uo_grad\n#                 for w in range(len(vocab)):\n#                     U[w, :] -= lr * uw_grad[w]\n                \n#                 pbar.update(1)  # Update progress bar per pair\n        \n#         avg_epoch_loss = float(epoch_loss / num_pairs)\n#         loss_history.append(avg_epoch_loss)\n#         pbar.set_postfix({'Epoch Loss': avg_epoch_loss})\n#         print(f\"Epoch {epoch}, Loss: {avg_epoch_loss}\")\n\n# pbar.close()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-06T14:09:59.126062Z","iopub.execute_input":"2025-04-06T14:09:59.126394Z","iopub.status.idle":"2025-04-06T14:09:59.658349Z","shell.execute_reply.started":"2025-04-06T14:09:59.126365Z","shell.execute_reply":"2025-04-06T14:09:59.657104Z"}},"outputs":[{"name":"stderr","text":"Training Skip-gram:   0%|          | 0/1000000 [00:00<?, ?it/s]\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)","\u001b[0;32m<ipython-input-34-5deddc4135e1>\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     22\u001b[0m                 \u001b[0mvc_grad\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0mu_o\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mcp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mp_w_c\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mU\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m                 \u001b[0muo_grad\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mp_w_c\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mo_idx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mv_c\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m                 \u001b[0muw_grad\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mp_w_c\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mv_c\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     25\u001b[0m                 \u001b[0muw_grad\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mo_idx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mp_w_c\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mo_idx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mv_c\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32mcupy/_core/core.pyx\u001b[0m in \u001b[0;36mcupy._core.core._ndarray_base.__mul__\u001b[0;34m()\u001b[0m\n","\u001b[0;32mcupy/_core/_kernel.pyx\u001b[0m in \u001b[0;36mcupy._core._kernel.ufunc.__call__\u001b[0;34m()\u001b[0m\n","\u001b[0;32mcupy/_core/internal.pyx\u001b[0m in \u001b[0;36mcupy._core.internal._broadcast_core\u001b[0;34m()\u001b[0m\n","\u001b[0;31mValueError\u001b[0m: operands could not be broadcast together with shapes (10000,) (100,)"],"ename":"ValueError","evalue":"operands could not be broadcast together with shapes (10000,) (100,)","output_type":"error"}],"execution_count":34},{"cell_type":"code","source":"V","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-06T09:43:19.214963Z","iopub.execute_input":"2025-04-06T09:43:19.215316Z","iopub.status.idle":"2025-04-06T09:43:19.221847Z","shell.execute_reply.started":"2025-04-06T09:43:19.215290Z","shell.execute_reply":"2025-04-06T09:43:19.220814Z"}},"outputs":[{"execution_count":30,"output_type":"execute_result","data":{"text/plain":"array([[ 1.40136926, -0.49552651, -0.42532298],\n       [ 0.05313453, -0.03351528,  0.68138598],\n       [-0.23847167, -0.40992499, -0.06977735],\n       [-0.09327522,  0.22551045, -0.07777905],\n       [ 0.29441731, -0.07724471, -0.13224349],\n       [ 0.32116448,  0.18530075,  1.10928156],\n       [-0.60548336, -1.4123885 ,  0.1138442 ]])"},"metadata":{}}],"execution_count":30},{"cell_type":"code","source":"vocab","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-06T09:43:30.186898Z","iopub.execute_input":"2025-04-06T09:43:30.187256Z","iopub.status.idle":"2025-04-06T09:43:30.192765Z","shell.execute_reply.started":"2025-04-06T09:43:30.187229Z","shell.execute_reply":"2025-04-06T09:43:30.191977Z"}},"outputs":[{"execution_count":31,"output_type":"execute_result","data":{"text/plain":"['The', 'cat', 'and', 'dog', 'had', 'a', 'fight']"},"metadata":{}}],"execution_count":31},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}