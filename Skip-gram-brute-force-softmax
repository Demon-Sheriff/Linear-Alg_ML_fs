{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[],"dockerImageVersionId":30918,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-04-10T10:42:03.165910Z","iopub.execute_input":"2025-04-10T10:42:03.166146Z","iopub.status.idle":"2025-04-10T10:42:03.170972Z","shell.execute_reply.started":"2025-04-10T10:42:03.166094Z","shell.execute_reply":"2025-04-10T10:42:03.169986Z"}},"outputs":[],"execution_count":39},{"cell_type":"code","source":"!pip install cupy-cuda12x","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-10T10:42:04.748782Z","iopub.execute_input":"2025-04-10T10:42:04.749089Z","iopub.status.idle":"2025-04-10T10:42:08.158529Z","shell.execute_reply.started":"2025-04-10T10:42:04.749064Z","shell.execute_reply":"2025-04-10T10:42:08.157626Z"}},"outputs":[{"name":"stdout","text":"Requirement already satisfied: cupy-cuda12x in /usr/local/lib/python3.10/dist-packages (12.2.0)\nRequirement already satisfied: numpy<1.27,>=1.20 in /usr/local/lib/python3.10/dist-packages (from cupy-cuda12x) (1.26.4)\nRequirement already satisfied: fastrlock>=0.5 in /usr/local/lib/python3.10/dist-packages (from cupy-cuda12x) (0.8.2)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.10/dist-packages (from numpy<1.27,>=1.20->cupy-cuda12x) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.10/dist-packages (from numpy<1.27,>=1.20->cupy-cuda12x) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.10/dist-packages (from numpy<1.27,>=1.20->cupy-cuda12x) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.10/dist-packages (from numpy<1.27,>=1.20->cupy-cuda12x) (2025.0.1)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.10/dist-packages (from numpy<1.27,>=1.20->cupy-cuda12x) (2022.0.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.10/dist-packages (from numpy<1.27,>=1.20->cupy-cuda12x) (2.4.1)\nRequirement already satisfied: intel-openmp>=2024 in /usr/local/lib/python3.10/dist-packages (from mkl->numpy<1.27,>=1.20->cupy-cuda12x) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.10/dist-packages (from mkl->numpy<1.27,>=1.20->cupy-cuda12x) (2022.0.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.10/dist-packages (from tbb==2022.*->mkl->numpy<1.27,>=1.20->cupy-cuda12x) (1.2.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.10/dist-packages (from mkl_umath->numpy<1.27,>=1.20->cupy-cuda12x) (2024.2.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.10/dist-packages (from intel-openmp>=2024->mkl->numpy<1.27,>=1.20->cupy-cuda12x) (2024.2.0)\n","output_type":"stream"}],"execution_count":40},{"cell_type":"code","source":"!nvcc --version","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-10T10:42:09.354507Z","iopub.execute_input":"2025-04-10T10:42:09.354837Z","iopub.status.idle":"2025-04-10T10:42:09.520143Z","shell.execute_reply.started":"2025-04-10T10:42:09.354802Z","shell.execute_reply":"2025-04-10T10:42:09.519228Z"}},"outputs":[{"name":"stdout","text":"nvcc: NVIDIA (R) Cuda compiler driver\nCopyright (c) 2005-2023 NVIDIA Corporation\nBuilt on Tue_Aug_15_22:02:13_PDT_2023\nCuda compilation tools, release 12.2, V12.2.140\nBuild cuda_12.2.r12.2/compiler.33191640_0\n","output_type":"stream"}],"execution_count":41},{"cell_type":"code","source":"import cupy as cp","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-10T10:42:10.491675Z","iopub.execute_input":"2025-04-10T10:42:10.491989Z","iopub.status.idle":"2025-04-10T10:42:10.496087Z","shell.execute_reply.started":"2025-04-10T10:42:10.491964Z","shell.execute_reply":"2025-04-10T10:42:10.495193Z"}},"outputs":[],"execution_count":42},{"cell_type":"code","source":"import gensim.downloader as api\ndataset = api.load(\"text8\")  # or \"text8\", \"wiki-english-20171001\", etc.","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-10T10:42:11.922237Z","iopub.execute_input":"2025-04-10T10:42:11.922564Z","iopub.status.idle":"2025-04-10T10:42:12.117257Z","shell.execute_reply.started":"2025-04-10T10:42:11.922535Z","shell.execute_reply":"2025-04-10T10:42:12.116486Z"}},"outputs":[],"execution_count":43},{"cell_type":"code","source":"sentences = [list(sent) for sent in dataset]\nwords = 0\nfor s in sentences:\n    words += len(s)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-10T10:42:15.968492Z","iopub.execute_input":"2025-04-10T10:42:15.968824Z","iopub.status.idle":"2025-04-10T10:42:19.224471Z","shell.execute_reply.started":"2025-04-10T10:42:15.968798Z","shell.execute_reply":"2025-04-10T10:42:19.223539Z"}},"outputs":[],"execution_count":44},{"cell_type":"code","source":"words # 17005207","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-10T10:42:23.015379Z","iopub.execute_input":"2025-04-10T10:42:23.015665Z","iopub.status.idle":"2025-04-10T10:42:23.020916Z","shell.execute_reply.started":"2025-04-10T10:42:23.015644Z","shell.execute_reply":"2025-04-10T10:42:23.020046Z"}},"outputs":[{"execution_count":45,"output_type":"execute_result","data":{"text/plain":"17005207"},"metadata":{}}],"execution_count":45},{"cell_type":"code","source":"flat_words = [word for sentence in sentences for word in sentence]\nlen(flat_words)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-10T10:42:24.433275Z","iopub.execute_input":"2025-04-10T10:42:24.433582Z","iopub.status.idle":"2025-04-10T10:42:25.277652Z","shell.execute_reply.started":"2025-04-10T10:42:24.433558Z","shell.execute_reply":"2025-04-10T10:42:25.276862Z"}},"outputs":[{"execution_count":46,"output_type":"execute_result","data":{"text/plain":"17005207"},"metadata":{}}],"execution_count":46},{"cell_type":"code","source":"len(set(flat_words)) # unique words in the complete corpus -> vocab size","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-10T10:42:26.973957Z","iopub.execute_input":"2025-04-10T10:42:26.974377Z","iopub.status.idle":"2025-04-10T10:42:28.086390Z","shell.execute_reply.started":"2025-04-10T10:42:26.974343Z","shell.execute_reply":"2025-04-10T10:42:28.085417Z"}},"outputs":[{"execution_count":47,"output_type":"execute_result","data":{"text/plain":"253854"},"metadata":{}}],"execution_count":47},{"cell_type":"code","source":"c = 0\nfor w in (\"queen\", \"king\", \"man\", \"woman\"):\n    if w in set(flat_words[:100000]):\n        c += 1\n    else:\n        print(f\"{w} not found in corpus\")\nif c == 4:\n    print(\"all words present int the corpus\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-10T10:42:29.294796Z","iopub.execute_input":"2025-04-10T10:42:29.295086Z","iopub.status.idle":"2025-04-10T10:42:29.315174Z","shell.execute_reply.started":"2025-04-10T10:42:29.295064Z","shell.execute_reply":"2025-04-10T10:42:29.314274Z"}},"outputs":[{"name":"stdout","text":"all words present int the corpus\n","output_type":"stream"}],"execution_count":48},{"cell_type":"code","source":"corpus = flat_words[:100000]\nvocab = set(flat_words[:100000])\nvocab_size = len(vocab)\nm = 4 # context window of 9 words [4 future and 4 history and 1 center word]\nd = 100 # starting out with 100 dimensionality for word vectors","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-10T11:01:58.465665Z","iopub.execute_input":"2025-04-10T11:01:58.465986Z","iopub.status.idle":"2025-04-10T11:01:58.475305Z","shell.execute_reply.started":"2025-04-10T11:01:58.465960Z","shell.execute_reply":"2025-04-10T11:01:58.474215Z"}},"outputs":[],"execution_count":65},{"cell_type":"code","source":"len(vocab), len(corpus)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-10T11:01:59.517268Z","iopub.execute_input":"2025-04-10T11:01:59.517542Z","iopub.status.idle":"2025-04-10T11:01:59.522721Z","shell.execute_reply.started":"2025-04-10T11:01:59.517521Z","shell.execute_reply":"2025-04-10T11:01:59.521947Z"}},"outputs":[{"execution_count":66,"output_type":"execute_result","data":{"text/plain":"(12023, 100000)"},"metadata":{}}],"execution_count":66},{"cell_type":"code","source":"total_iterations = epochs * len(corpus) * (2 * m)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-10T11:02:00.205534Z","iopub.execute_input":"2025-04-10T11:02:00.205847Z","iopub.status.idle":"2025-04-10T11:02:00.209504Z","shell.execute_reply.started":"2025-04-10T11:02:00.205821Z","shell.execute_reply":"2025-04-10T11:02:00.208646Z"}},"outputs":[],"execution_count":67},{"cell_type":"code","source":"# Init embeddings\nU = cp.random.normal(0, 1.0/d, (vocab_size, d))  # Context\nV = cp.random.normal(0, 1.0/d, (vocab_size, d))  # Center","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-10T11:02:01.366468Z","iopub.execute_input":"2025-04-10T11:02:01.366765Z","iopub.status.idle":"2025-04-10T11:02:01.371303Z","shell.execute_reply.started":"2025-04-10T11:02:01.366742Z","shell.execute_reply":"2025-04-10T11:02:01.370394Z"}},"outputs":[],"execution_count":68},{"cell_type":"code","source":"from tqdm import tqdm","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-10T11:02:02.692060Z","iopub.execute_input":"2025-04-10T11:02:02.692457Z","iopub.status.idle":"2025-04-10T11:02:02.696299Z","shell.execute_reply.started":"2025-04-10T11:02:02.692429Z","shell.execute_reply":"2025-04-10T11:02:02.695392Z"}},"outputs":[],"execution_count":69},{"cell_type":"code","source":"epochs = 20 # starting with 20 epochs for now.\nlr = 0.05 # learning rate","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-10T11:02:03.952409Z","iopub.execute_input":"2025-04-10T11:02:03.952738Z","iopub.status.idle":"2025-04-10T11:02:03.956544Z","shell.execute_reply.started":"2025-04-10T11:02:03.952712Z","shell.execute_reply":"2025-04-10T11:02:03.955769Z"}},"outputs":[],"execution_count":70},{"cell_type":"code","source":"# Use the softmax trick\n# Apply gradient clipping\n# Apply parameter updates for best loss encountered\n# Apply early stopping\n# Applying subsampling\n# Applying learning rate decay","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-10T11:02:06.052540Z","iopub.execute_input":"2025-04-10T11:02:06.052853Z","iopub.status.idle":"2025-04-10T11:02:06.056366Z","shell.execute_reply.started":"2025-04-10T11:02:06.052829Z","shell.execute_reply":"2025-04-10T11:02:06.055459Z"}},"outputs":[],"execution_count":71},{"cell_type":"code","source":"loss_history = []\nbest_loss = float('inf')\nU_best = None\nV_best = None","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-10T11:02:07.387220Z","iopub.execute_input":"2025-04-10T11:02:07.387542Z","iopub.status.idle":"2025-04-10T11:02:07.391367Z","shell.execute_reply.started":"2025-04-10T11:02:07.387518Z","shell.execute_reply":"2025-04-10T11:02:07.390403Z"}},"outputs":[],"execution_count":72},{"cell_type":"code","source":"# prepare training pairs \nword_to_index = {word: idx for idx, word in enumerate(vocab)}\nindex_to_word = {idx: word for word, idx in word_to_index.items()}\n\ncorpus_ids = [word_to_index[word] for word in corpus]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-10T11:02:08.406695Z","iopub.execute_input":"2025-04-10T11:02:08.406974Z","iopub.status.idle":"2025-04-10T11:02:08.427194Z","shell.execute_reply.started":"2025-04-10T11:02:08.406954Z","shell.execute_reply":"2025-04-10T11:02:08.426314Z"}},"outputs":[],"execution_count":73},{"cell_type":"code","source":"with tqdm(total=total_iterations, desc=\"Training Skip-gram\") as pbar:\n    print(\"जय बजरंग बली\") # ॐ \n    for epoch in tqdm(range(epochs)):\n\n        epoch_loss = 0\n        # run through the corpus treating each word as a center word\n        for c_idx, center_word_id in enumerate(corpus_ids):\n            v_c = V[center_word_id, :]\n    \n            # iterate over the context window to find context|center (o|c) pairs\n            # context window would be from [c_idx - m, c_idx + m] excluding c_idx\n            # left boundary = max(0, c_idx - m), right boundary = min(corpus_size - 1, c_idx + m)\n            context_words = [idx for idx in range(max(0, c_idx - m), min(len(corpus_ids), c_idx + m + 1)) if idx != c_idx]\n            \n            for o_idx in context_words:\n                context_word_id = corpus_ids[o_idx]\n                # pair (o|c) -> (o_idx | c_idx)\n                u_o = U[context_word_id, :]\n                \n                # calculate softmax probability -> p(o|c)\n                dot_prods = cp.dot(U, v_c) # dot prodcuts of the entire context embedding wrods with the current center word\n                max_dot = cp.max(dot_prods) # max of the dot products for the softmax trick\n                smx_denom = cp.sum(cp.exp(dot_prods - max_dot)) # using the softmax trick\n                curr_dot = cp.dot(u_o, v_c) # dot product of the current center and context words\n                smx_num = cp.exp(curr_dot - max_dot)\n                p_o_c = smx_num/smx_denom \n    \n                # current loss of o and c is the log softmax prob\n                # log(smx_num) = curr_dot\n                curr_loss = -curr_dot + max_dot + cp.log(cp.sum(cp.exp(dot_prods - max_dot)))\n                epoch_loss += curr_loss\n    \n                # calculate p(w | c)\n                p_w_c = cp.exp(dot_prods - max_dot)/smx_denom\n                \n                # compute the gradients \n                # for v_c (center word) [calculated with the softmax trick]\n                grad_v_c = -u_o + cp.dot(p_w_c, U)\n    \n                # for u_w (other context words excluding the current context word)\n                # print(p_w_c.shape, v_c.shape)\n                grad_u_w = cp.outer(p_w_c, v_c)\n                grad_u_w[context_word_id, :] = 0\n    \n                # for u_o (current context word)\n                grad_u_o = v_c * (p_o_c - 1)\n\n                # update the parameters\n                V[center_word_id, :] -= lr * (grad_v_c)\n                U -= lr * (grad_u_w)\n                U[context_word_id, :] -= lr * (grad_u_o)\n            \n        avg_epoch_loss = epoch_loss / (len(corpus) * 2 * m)  # rough estimate of pairs\n        loss_history.append(avg_epoch_loss)\n        print(f\"Epoch {epoch + 1}, Loss: {avg_epoch_loss}\")\n\n        # save the best parameters\n        if avg_epoch_loss < best_loss:\n            U_best = U.copy()\n            V_best = V.copy()\n            best_loss = avg_epoch_loss\n                \npbar.close()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-10T11:02:10.358763Z","iopub.execute_input":"2025-04-10T11:02:10.359123Z","iopub.status.idle":"2025-04-10T14:23:55.850228Z","shell.execute_reply.started":"2025-04-10T11:02:10.359071Z","shell.execute_reply":"2025-04-10T14:23:55.849439Z"}},"outputs":[{"name":"stderr","text":"Training Skip-gram:   0%|          | 0/16000000 [00:00<?, ?it/s]","output_type":"stream"},{"name":"stdout","text":"जय बजरंग बली\n","output_type":"stream"},{"name":"stderr","text":"\n  0%|          | 0/20 [00:00<?, ?it/s]\u001b[A","output_type":"stream"},{"name":"stdout","text":"Epoch 0, Loss: 6.769050345657568\n","output_type":"stream"},{"name":"stderr","text":"\n  5%|▌         | 1/20 [10:07<3:12:18, 607.31s/it]\u001b[A\n 10%|█         | 2/20 [20:10<3:01:27, 604.88s/it]\u001b[A","output_type":"stream"},{"name":"stdout","text":"Epoch 1, Loss: 6.297657898846818\n","output_type":"stream"},{"name":"stderr","text":"\n 15%|█▌        | 3/20 [30:17<2:51:41, 606.00s/it]\u001b[A","output_type":"stream"},{"name":"stdout","text":"Epoch 2, Loss: 6.0868784080538605\n","output_type":"stream"},{"name":"stderr","text":"\n 20%|██        | 4/20 [40:21<2:41:18, 604.93s/it]\u001b[A","output_type":"stream"},{"name":"stdout","text":"Epoch 3, Loss: 5.9034091427176\n","output_type":"stream"},{"name":"stderr","text":"\n 25%|██▌       | 5/20 [50:24<2:31:06, 604.40s/it]\u001b[A","output_type":"stream"},{"name":"stdout","text":"Epoch 4, Loss: 5.7360555533027116\n","output_type":"stream"},{"name":"stderr","text":"\n 30%|███       | 6/20 [1:00:28<2:20:57, 604.14s/it]\u001b[A","output_type":"stream"},{"name":"stdout","text":"Epoch 5, Loss: 5.586203718928743\n","output_type":"stream"},{"name":"stderr","text":"\n 35%|███▌      | 7/20 [1:10:30<2:10:44, 603.39s/it]\u001b[A","output_type":"stream"},{"name":"stdout","text":"Epoch 6, Loss: 5.456922839869485\n","output_type":"stream"},{"name":"stderr","text":"\n 40%|████      | 8/20 [1:20:33<2:00:39, 603.33s/it]\u001b[A","output_type":"stream"},{"name":"stdout","text":"Epoch 7, Loss: 5.349607421434343\n","output_type":"stream"},{"name":"stderr","text":"\n 45%|████▌     | 9/20 [1:30:34<1:50:29, 602.71s/it]\u001b[A","output_type":"stream"},{"name":"stdout","text":"Epoch 8, Loss: 5.264393497579107\n","output_type":"stream"},{"name":"stderr","text":"\n 50%|█████     | 10/20 [1:40:36<1:40:24, 602.41s/it]\u001b[A","output_type":"stream"},{"name":"stdout","text":"Epoch 9, Loss: 5.199751967431902\n","output_type":"stream"},{"name":"stderr","text":"\n 55%|█████▌    | 11/20 [1:50:40<1:30:26, 602.99s/it]\u001b[A","output_type":"stream"},{"name":"stdout","text":"Epoch 10, Loss: 5.1523240995410164\n","output_type":"stream"},{"name":"stderr","text":"\n 60%|██████    | 12/20 [2:00:44<1:20:26, 603.31s/it]\u001b[A","output_type":"stream"},{"name":"stdout","text":"Epoch 11, Loss: 5.11806300842657\n","output_type":"stream"},{"name":"stderr","text":"\n 65%|██████▌   | 13/20 [2:10:47<1:10:21, 603.13s/it]\u001b[A","output_type":"stream"},{"name":"stdout","text":"Epoch 12, Loss: 5.093342040938001\n","output_type":"stream"},{"name":"stderr","text":"\n 70%|███████   | 14/20 [2:20:51<1:00:21, 603.53s/it]\u001b[A","output_type":"stream"},{"name":"stdout","text":"Epoch 13, Loss: 5.075354421588178\n","output_type":"stream"},{"name":"stderr","text":"\n 75%|███████▌  | 15/20 [2:30:58<50:22, 604.42s/it]  \u001b[A","output_type":"stream"},{"name":"stdout","text":"Epoch 14, Loss: 5.06208057912737\n","output_type":"stream"},{"name":"stderr","text":"\n 80%|████████  | 16/20 [2:41:05<40:21, 605.34s/it]\u001b[A","output_type":"stream"},{"name":"stdout","text":"Epoch 15, Loss: 5.052120872756245\n","output_type":"stream"},{"name":"stderr","text":"\n 85%|████████▌ | 17/20 [2:51:19<30:23, 607.82s/it]\u001b[A","output_type":"stream"},{"name":"stdout","text":"Epoch 16, Loss: 5.044526918037878\n","output_type":"stream"},{"name":"stderr","text":"\n 90%|█████████ | 18/20 [3:01:28<20:16, 608.05s/it]\u001b[A","output_type":"stream"},{"name":"stdout","text":"Epoch 17, Loss: 5.038659482468518\n","output_type":"stream"},{"name":"stderr","text":"\n 95%|█████████▌| 19/20 [3:11:37<10:08, 608.38s/it]\u001b[A","output_type":"stream"},{"name":"stdout","text":"Epoch 18, Loss: 5.0340836955896755\n","output_type":"stream"},{"name":"stderr","text":"\n100%|██████████| 20/20 [3:21:45<00:00, 605.25s/it]\u001b[A\nTraining Skip-gram:   0%|          | 0/16000000 [3:21:45<?, ?it/s]","output_type":"stream"},{"name":"stdout","text":"Epoch 19, Loss: 5.03049758047261\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"}],"execution_count":74},{"cell_type":"code","source":"# Save the best parameters\ncp.save('U_best.npy', U_best)\ncp.save('V_best.npy', V_best)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-10T14:24:30.184267Z","iopub.execute_input":"2025-04-10T14:24:30.184558Z","iopub.status.idle":"2025-04-10T14:24:30.210703Z","shell.execute_reply.started":"2025-04-10T14:24:30.184538Z","shell.execute_reply":"2025-04-10T14:24:30.210015Z"}},"outputs":[],"execution_count":76},{"cell_type":"code","source":"V","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-10T14:24:35.502192Z","iopub.execute_input":"2025-04-10T14:24:35.502490Z","iopub.status.idle":"2025-04-10T14:24:35.510082Z","shell.execute_reply.started":"2025-04-10T14:24:35.502468Z","shell.execute_reply":"2025-04-10T14:24:35.509205Z"}},"outputs":[{"execution_count":77,"output_type":"execute_result","data":{"text/plain":"array([[ 0.12586048, -0.47599888,  0.40659879, ..., -0.8628752 ,\n        -0.50047947, -0.54027686],\n       [-0.05374983, -0.66284316,  0.26385584, ...,  0.41912639,\n        -0.9457773 , -0.13180737],\n       [ 0.51299343,  0.3699927 , -0.67144755, ...,  0.1189268 ,\n        -0.85341802,  0.13788434],\n       ...,\n       [ 0.54072445, -0.39367576, -0.28051023, ..., -0.79629135,\n        -1.1181213 ,  0.71695392],\n       [-0.26540997, -0.0273611 ,  0.28402652, ..., -0.17338503,\n        -1.05298493,  0.32804458],\n       [ 0.59623717,  0.01347175,  0.05034263, ..., -0.62305279,\n        -1.05129148, -0.4418023 ]])"},"metadata":{}}],"execution_count":77},{"cell_type":"code","source":"U","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-10T14:24:39.727128Z","iopub.execute_input":"2025-04-10T14:24:39.727434Z","iopub.status.idle":"2025-04-10T14:24:39.734755Z","shell.execute_reply.started":"2025-04-10T14:24:39.727410Z","shell.execute_reply":"2025-04-10T14:24:39.734047Z"}},"outputs":[{"execution_count":78,"output_type":"execute_result","data":{"text/plain":"array([[-1.08837284,  0.83821931, -0.0823049 , ..., -0.93364504,\n        -0.7503726 , -0.59731123],\n       [ 0.02825276, -0.32903795, -0.24986923, ...,  0.10908173,\n        -0.59173728, -0.12835823],\n       [-0.18938615, -0.55229839, -0.12309663, ..., -1.22442194,\n        -1.1340168 , -0.4241466 ],\n       ...,\n       [ 1.23483899, -0.85320216, -0.1969191 , ..., -0.24815365,\n        -0.7731523 ,  0.2664228 ],\n       [ 0.14286476, -0.50760902,  0.64535692, ..., -0.37850286,\n         0.3383721 , -0.13827356],\n       [ 0.18787779, -0.07109835,  0.50021226, ..., -0.45847908,\n        -0.82850684, -0.32834168]])"},"metadata":{}}],"execution_count":78},{"cell_type":"code","source":"def analogy(a, b, c, V_best, word_to_index, index_to_word, top_k=5):\n    # Convert words to embeddings\n    vec_a = V_best[word_to_index[a]]\n    vec_b = V_best[word_to_index[b]]\n    vec_c = V_best[word_to_index[c]]\n\n    # Compute target embedding\n    target_vec = vec_b - vec_a + vec_c\n\n    # Normalize\n    V_norm = V_best / cp.linalg.norm(V_best, axis=1, keepdims=True)\n    target_vec /= cp.linalg.norm(target_vec)\n\n    # Cosine similarity\n    similarities = cp.dot(V_norm, target_vec)\n\n    # Top-k excluding a, b, c\n    top_ids = cp.argsort(similarities)[::-1]\n    results = []\n\n    for idx in top_ids:\n        word = index_to_word[int(idx)]\n        if word not in {a, b, c}:\n            results.append((word, float(similarities[idx])))\n        if len(results) == top_k:\n            break\n\n    return results","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-10T14:26:17.567767Z","iopub.execute_input":"2025-04-10T14:26:17.568069Z","iopub.status.idle":"2025-04-10T14:26:17.573901Z","shell.execute_reply.started":"2025-04-10T14:26:17.568048Z","shell.execute_reply":"2025-04-10T14:26:17.572928Z"}},"outputs":[],"execution_count":81},{"cell_type":"code","source":"result = analogy(\"king\", \"man\", \"woman\", V_best, word_to_index, index_to_word)\n\nprint(\"\\nAnalogy: king - man + woman ≈ ?\")\nfor word, sim in result:\n    print(f\"{word} (score: {sim:.4f})\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-10T14:26:27.127984Z","iopub.execute_input":"2025-04-10T14:26:27.128341Z","iopub.status.idle":"2025-04-10T14:26:32.949797Z","shell.execute_reply.started":"2025-04-10T14:26:27.128315Z","shell.execute_reply":"2025-04-10T14:26:32.949013Z"}},"outputs":[{"name":"stdout","text":"\nAnalogy: king - man + woman ≈ ?\nsoul (score: 0.4339)\ninvestigates (score: 0.4295)\nhack (score: 0.4285)\npork (score: 0.4269)\nqua (score: 0.4242)\n","output_type":"stream"}],"execution_count":82},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}