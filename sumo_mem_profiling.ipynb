{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyOLM/AcgSHkHpJfg6Q0ecUe",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Demon-Sheriff/Linear-Alg_ML_fs/blob/master/sumo_mem_profiling.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "nJOu0Mp6wPHc"
      },
      "outputs": [],
      "source": [
        "# import torch\n",
        "\n",
        "# def frag_report():\n",
        "#     stats = torch.cuda.memory_stats()\n",
        "#     print(\"Total allocated: \", stats[\"allocated_bytes.all.current\"] / 1e9, \"GB\")\n",
        "#     print(\"Total reserved : \", stats[\"reserved_bytes.all.current\"] / 1e9, \"GB\")\n",
        "#     print(\"Active bytes   : \", stats[\"active_bytes.all.current\"] / 1e9, \"GB\")\n",
        "#     print(\"Inactive split : \", stats[\"inactive_split_bytes.all.current\"] / 1e9, \"GB\")\n",
        "#     print(\"Largest block  : \", stats[\"largest_block_bytes.all.current\"] / 1e9, \"GB\")\n",
        "\n",
        "# frag_report()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# %env PYTORCH_CUDA_ALLOC_CONF=backend:cudaMallocAsync\n",
        "# %env PYTORCH_CUDA_ALLOC_CONF=max_split_size_mb:128\n",
        "%env PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AOnaXFbTQsTh",
        "outputId": "11fc5d2c-da17-434e-c23f-b5a957727185"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "env: PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch"
      ],
      "metadata": {
        "id": "oKHcS1JoQ5a7"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "torch.cuda.memory._record_memory_history()"
      ],
      "metadata": {
        "id": "Zt9ISVuryfaP"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "@torch.compile\n",
        "def orthogonalization_svd(M):\n",
        "  U,sig,V = torch.linalg.svd(M.float(), full_matrices=False) # TODO: figuring out full_matrices=True case\n",
        "  return U @ V\n",
        "\n",
        "class SUMO(torch.optim.Optimizer):\n",
        "  def __init__(self, params, lr=1e-4, scale_factor=2.0, weight_decay=0.01,\n",
        "                rank=8, K=50, step_clip_ratio=1.1, momentum_coeff=0.9):\n",
        "      defaults = dict(lr=lr, scale_factor=scale_factor, weight_decay=weight_decay,\n",
        "                      step_clip_ratio=step_clip_ratio, momentum_coeff=momentum_coeff, rank=rank, K=K)\n",
        "      super(SUMO, self).__init__(params, defaults)\n",
        "\n",
        "  def step(self):\n",
        "    for group in self.param_groups:\n",
        "      lr = group['lr']\n",
        "      scale_factor = group['scale_factor']\n",
        "      weight_decay = group['weight_decay']\n",
        "      rank = group['rank']\n",
        "      K = group['K']\n",
        "      step_clip_ratio = group['step_clip_ratio']\n",
        "      momentum_coeff = group['momentum_coeff']\n",
        "\n",
        "      # mul_, add_ and other inplace ops would only be used when manipulating internal gradient params.\n",
        "      for p in group['params']:\n",
        "        g = p.grad # the gradient matrix for w_t-1\n",
        "        if g is None: continue\n",
        "        state = self.state[p] # current state of the optimizer\n",
        "\n",
        "        if g.dim() < 2: # logic for handling bias/layernorm like params (< 2 dim)\n",
        "           # simple sgd + weight decay fallback\n",
        "           p.data.mul_(1 - lr * weight_decay)\n",
        "           p.data.add_(p.grad, alpha=-lr)\n",
        "           continue\n",
        "        if 'step' not in state.keys(): state['step'] = 0\n",
        "        _, _, m, n = g.size()\n",
        "        # m >= n assumption without loss of generality. [if n > m we transpose]\n",
        "        if (rev:=(m<n)) : g = g.transpose(-2, -1); _, _, m, n = g.size()\n",
        "        if state['step'] % K == 0:\n",
        "          if rank >= (k:=min(m, n)): U_float, sig, V = torch.linalg.svd(g.float(), full_matrices=False) # full rank case : U shape : (m, n)\n",
        "          else: U_float, sig, V = torch.svd_lowrank(g.float(), q=rank) # U (m, r)\n",
        "\n",
        "          if 'sub_moment_buffer' not in state.keys():\n",
        "            # first step so no rotations. [init the bufs]\n",
        "            state['sub_moment_buffer'] = U_float\n",
        "            state['moment_buffer'] = torch.zeros(U_float.size(-1), n, device=g.device, dtype=torch.float)\n",
        "            state['orthogonal_buf'] = torch.zeros(U_float.size(-1), n, device=g.device, dtype=torch.float)\n",
        "            moment = state['moment_buffer']\n",
        "          else:\n",
        "            # apply rotations if not first step\n",
        "            R = U_float.transpose(-2, -1) @ state['sub_moment_buffer'] # Q_t @ Q_t-1\n",
        "            moment = R @ state['moment_buffer'] # use rotated moment for current update\n",
        "            state['sub_moment_buffer'] = U_float\n",
        "        else:\n",
        "          U_float = state['sub_moment_buffer']\n",
        "          moment = state['moment_buffer']\n",
        "        U_for_matmul = U_float.to(g.dtype)\n",
        "        G_hat = U_for_matmul.transpose(-2, -1) @ g\n",
        "        M_float = momentum_coeff * moment + G_hat.float()\n",
        "        state['moment_buffer'] = M_float\n",
        "        O_float = orthogonalization_svd(M_float) # (r, n) if r <= k else (k, n)\n",
        "\n",
        "        if (prev_o_norm:=torch.norm(state['orthogonal_buf'])) > 1e-8 and ((o_norm:=torch.norm(O_float)) > step_clip_ratio * prev_o_norm):\n",
        "          O_float = (O_float * (step_clip_ratio * prev_o_norm)) / o_norm # (r, n) if r <= k else (k, n)\n",
        "        # update orthogonal buffer\n",
        "        state['orthogonal_buf'] = O_float\n",
        "\n",
        "        # weight updates\n",
        "        update_direction = g - (U_for_matmul @ (G_hat - O_float.to(g.dtype))).view_as(g)\n",
        "        p.data.mul_(1 - lr * weight_decay) # apply weight decay to the weigts\n",
        "        p.data.add_(scale_factor * (update_direction.transpose(-2, -1)) if rev else scale_factor * update_direction, alpha=-lr)\n",
        "        state['step'] += 1"
      ],
      "metadata": {
        "id": "bHHThiq20eBc"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "stats = torch.cuda.memory_stats()"
      ],
      "metadata": {
        "id": "Hc-rY0E_yBsq"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for k,v in stats.keys():\n",
        "  print(k)"
      ],
      "metadata": {
        "id": "F6w93hpMyFqs"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import wandb\n",
        "import sys\n",
        "with open(sys.argv[0]) as f:\n",
        "    code = f.read()\n",
        "import uuid\n",
        "from math import ceil\n",
        "\n",
        "# import torch\n",
        "from torch import nn\n",
        "import torch.nn.functional as F\n",
        "import torchvision\n",
        "import torchvision.transforms as T\n",
        "\n",
        "torch.backends.cudnn.benchmark = True\n",
        "\n",
        "#############################################\n",
        "#               SUMO optimizer              #\n",
        "#############################################\n",
        "\n",
        "@torch.compile\n",
        "def orthogonalization_svd(M):\n",
        "  U,sig,V = torch.linalg.svd(M.float(), full_matrices=False) # TODO: figuring out full_matrices=True case\n",
        "  return U @ V\n",
        "\n",
        "class SUMO(torch.optim.Optimizer):\n",
        "  def __init__(self, params, lr=1e-4, scale_factor=2.0, weight_decay=0.01,\n",
        "                rank=8, K=200, step_clip_ratio=1.1, momentum_coeff=0.9):\n",
        "      defaults = dict(lr=lr, scale_factor=scale_factor, weight_decay=weight_decay,\n",
        "                      step_clip_ratio=step_clip_ratio, momentum_coeff=momentum_coeff, rank=rank, K=K)\n",
        "      super(SUMO, self).__init__(params, defaults)\n",
        "\n",
        "  def __setstate__(self, state):\n",
        "    super(SUMO, self).__setstate__(state)\n",
        "\n",
        "  def step(self):\n",
        "    for group in self.param_groups:\n",
        "      # step_size = group['lr']\n",
        "      lr = group['lr']\n",
        "      scale_factor = group['scale_factor']\n",
        "      weight_decay = group['weight_decay']\n",
        "      rank = group['rank']\n",
        "      K = group['K']\n",
        "      step_clip_ratio = group['step_clip_ratio']\n",
        "      momentum_coeff = group['momentum_coeff']\n",
        "\n",
        "      # mul_, add_ and other inplace ops would only be used when manipulating internal gradient params.\n",
        "      for p in group['params']:\n",
        "        g = p.grad # the gradient matrix for w_t-1\n",
        "        if g is None: continue\n",
        "        state = self.state[p] # current state of the optimizer\n",
        "\n",
        "        if g.dim() < 2: # logic for handling bias/layernorm like params\n",
        "           # simple sgd + weight decay fallback\n",
        "           p.data.mul_(1 - lr * weight_decay)\n",
        "           p.data.add_(p.grad, alpha=-lr)\n",
        "           continue\n",
        "\n",
        "        if 'step' not in state.keys(): state['step'] = 0\n",
        "        # print(g.dim())\n",
        "        _, _, m, n = g.size()\n",
        "        # m >= n assumption without loss of generality. [if n > m we transpose]\n",
        "        if (rev:=(m<n)) : g = g.transpose(-2, -1); _, _, m, n = g.size()\n",
        "\n",
        "        if state['step'] % K == 0:\n",
        "          if rank >= (k:=min(m, n)): U_float, sig, V = torch.linalg.svd(g.float(), full_matrices=False) # full rank case : U shape : (m, n)\n",
        "          else: U_float, sig, V = torch.svd_lowrank(g.float(), q=rank) # U (m, r)\n",
        "\n",
        "          if 'sub_moment_buffer' not in state.keys():\n",
        "            # first step so no rotations. [init the bufs]\n",
        "            state['sub_moment_buffer'] = U_float # why init with U?\n",
        "            state['moment_buffer'] = torch.zeros(U_float.size(-1), n, device=g.device, dtype=torch.float) # why not init identity ? and why use dynamic U shape calc ? wouldn't U shapes remain fixed.\n",
        "            state['orthogonal_buf'] = torch.zeros(U_float.size(-1), n, device=g.device, dtype=torch.float) # same qn : why torch.eye ?\n",
        "\n",
        "            moment = state['moment_buffer']\n",
        "          else:\n",
        "            # apply rotations if not first step\n",
        "            R = U_float.transpose(-2, -1) @ state['sub_moment_buffer'] # Q_t @ Q_t-1\n",
        "            moment = R @ state['moment_buffer'] # use rotated moment for current update\n",
        "            state['sub_moment_buffer'] = U_float\n",
        "\n",
        "        else:\n",
        "          U_float = state['sub_moment_buffer']\n",
        "          moment = state['moment_buffer']\n",
        "        # Cast U to g.dtype (e.g., half) for matrix multiplication with g\n",
        "        U_for_matmul = U_float.to(g.dtype)\n",
        "\n",
        "        # G_hat will be g.dtype (e.g., half)\n",
        "        G_hat = U_for_matmul.transpose(-2, -1) @ g\n",
        "\n",
        "        # Accumulate moment in float to keep moment_buffer float\n",
        "        # G_hat.float() is crucial here to match dtype of moment\n",
        "        M_float = momentum_coeff * moment + G_hat.float()\n",
        "        state['moment_buffer'] = M_float # Update moment_buffer as float\n",
        "\n",
        "        # O_float is computed from M_float, so O_float is float\n",
        "        O_float = orthogonalization_svd(M_float) # (r, n) if r <= k else (k, n)\n",
        "\n",
        "        # prevent zeroing out o_norm initially\n",
        "        if (prev_o_norm:=torch.norm(state['orthogonal_buf'])) > 1e-8 and ((o_norm:=torch.norm(O_float)) > step_clip_ratio * prev_o_norm):\n",
        "          O_float = (O_float * (step_clip_ratio * prev_o_norm)) / o_norm # (r, n) if r <= k else (k, n)\n",
        "\n",
        "        # update orthogonal buffer\n",
        "        state['orthogonal_buf'] = O_float\n",
        "\n",
        "        # weight updates\n",
        "        update_direction = g - (U_for_matmul @ (G_hat - O_float.to(g.dtype))).view_as(g)\n",
        "        # try:\n",
        "        #   update_direction = g - (U@(G_hat - O)).view_as(g)\n",
        "        # except Exception as e:\n",
        "        #   print(f\"G_hat shape: {G_hat.shape}, O shape: {O.shape}, g shape: {g.shape}, U shape: {U.shape}, M shape: {M.shape}\")\n",
        "\n",
        "        p.data.mul_(1 - lr * weight_decay) # apply weight decay to the weigts\n",
        "        update = scale_factor * update_direction\n",
        "        p.data.add_(update.transpose(-2, -1) if rev else update, alpha=-lr)\n",
        "        # update = lr*weight_decay*g + scale_factor*lr*(g - (U@(G_hat - O)).view_as(g))\n",
        "        # p.data.add_(update.transpose(-2, -1) if rev else update, alpha=-1.0)\n",
        "\n",
        "        state['step'] += 1\n",
        "\n",
        "#############################################\n",
        "#                DataLoader                 #\n",
        "#############################################\n",
        "\n",
        "CIFAR_MEAN = torch.tensor((0.4914, 0.4822, 0.4465))\n",
        "CIFAR_STD = torch.tensor((0.2470, 0.2435, 0.2616))\n",
        "\n",
        "def batch_flip_lr(inputs):\n",
        "    flip_mask = (torch.rand(len(inputs), device=inputs.device) < 0.5).view(-1, 1, 1, 1)\n",
        "    return torch.where(flip_mask, inputs.flip(-1), inputs)\n",
        "\n",
        "def batch_crop(images, crop_size):\n",
        "    r = (images.size(-1) - crop_size)//2\n",
        "    shifts = torch.randint(-r, r+1, size=(len(images), 2), device=images.device)\n",
        "    images_out = torch.empty((len(images), 3, crop_size, crop_size), device=images.device, dtype=images.dtype)\n",
        "    # The two cropping methods in this if-else produce equivalent results, but the second is faster for r > 2.\n",
        "    if r <= 2:\n",
        "        for sy in range(-r, r+1):\n",
        "            for sx in range(-r, r+1):\n",
        "                mask = (shifts[:, 0] == sy) & (shifts[:, 1] == sx)\n",
        "                images_out[mask] = images[mask, :, r+sy:r+sy+crop_size, r+sx:r+sx+crop_size]\n",
        "    else:\n",
        "        images_tmp = torch.empty((len(images), 3, crop_size, crop_size+2*r), device=images.device, dtype=images.dtype)\n",
        "        for s in range(-r, r+1):\n",
        "            mask = (shifts[:, 0] == s)\n",
        "            images_tmp[mask] = images[mask, :, r+s:r+s+crop_size, :]\n",
        "        for s in range(-r, r+1):\n",
        "            mask = (shifts[:, 1] == s)\n",
        "            images_out[mask] = images_tmp[mask, :, :, r+s:r+s+crop_size]\n",
        "    return images_out\n",
        "\n",
        "class CifarLoader:\n",
        "\n",
        "    def __init__(self, path, train=True, batch_size=500, aug=None):\n",
        "        data_path = os.path.join(path, 'train.pt' if train else 'test.pt')\n",
        "        if not os.path.exists(data_path):\n",
        "            dset = torchvision.datasets.CIFAR10(path, download=True, train=train)\n",
        "            images = torch.tensor(dset.data)\n",
        "            labels = torch.tensor(dset.targets)\n",
        "            torch.save({'images': images, 'labels': labels, 'classes': dset.classes}, data_path)\n",
        "\n",
        "        data = torch.load(data_path, map_location=torch.device('cuda'))\n",
        "        self.images, self.labels, self.classes = data['images'], data['labels'], data['classes']\n",
        "        # It's faster to load+process uint8 data than to load preprocessed fp16 data\n",
        "        self.images = (self.images.half() / 255).permute(0, 3, 1, 2).to(memory_format=torch.channels_last)\n",
        "\n",
        "        self.normalize = T.Normalize(CIFAR_MEAN, CIFAR_STD)\n",
        "        self.proc_images = {} # Saved results of image processing to be done on the first epoch\n",
        "        self.epoch = 0\n",
        "\n",
        "        self.aug = aug or {}\n",
        "        for k in self.aug.keys():\n",
        "            assert k in ['flip', 'translate'], 'Unrecognized key: %s' % k\n",
        "\n",
        "        self.batch_size = batch_size\n",
        "        self.drop_last = train\n",
        "        self.shuffle = train\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.images)//self.batch_size if self.drop_last else ceil(len(self.images)/self.batch_size)\n",
        "\n",
        "    def __iter__(self):\n",
        "\n",
        "        if self.epoch == 0:\n",
        "            images = self.proc_images['norm'] = self.normalize(self.images)\n",
        "            # Pre-flip images in order to do every-other epoch flipping scheme\n",
        "            if self.aug.get('flip', False):\n",
        "                images = self.proc_images['flip'] = batch_flip_lr(images)\n",
        "            # Pre-pad images to save time when doing random translation\n",
        "            pad = self.aug.get('translate', 0)\n",
        "            if pad > 0:\n",
        "                self.proc_images['pad'] = F.pad(images, (pad,)*4, 'reflect')\n",
        "\n",
        "        if self.aug.get('translate', 0) > 0:\n",
        "            images = batch_crop(self.proc_images['pad'], self.images.shape[-2])\n",
        "        elif self.aug.get('flip', False):\n",
        "            images = self.proc_images['flip']\n",
        "        else:\n",
        "            images = self.proc_images['norm']\n",
        "        # Flip all images together every other epoch. This increases diversity relative to random flipping\n",
        "        if self.aug.get('flip', False):\n",
        "            if self.epoch % 2 == 1:\n",
        "                images = images.flip(-1)\n",
        "\n",
        "        self.epoch += 1\n",
        "\n",
        "        indices = (torch.randperm if self.shuffle else torch.arange)(len(images), device=images.device)\n",
        "        for i in range(len(self)):\n",
        "            idxs = indices[i*self.batch_size:(i+1)*self.batch_size]\n",
        "            yield (images[idxs], self.labels[idxs])\n",
        "\n",
        "#############################################\n",
        "#            Network Definition             #\n",
        "#############################################\n",
        "\n",
        "# note the use of low BatchNorm stats momentum\n",
        "class BatchNorm(nn.BatchNorm2d):\n",
        "    def __init__(self, num_features, momentum=0.6, eps=1e-12):\n",
        "        super().__init__(num_features, eps=eps, momentum=1-momentum)\n",
        "        self.weight.requires_grad = False\n",
        "        # Note that PyTorch already initializes the weights to one and bias to zero\n",
        "\n",
        "class Conv(nn.Conv2d):\n",
        "    def __init__(self, in_channels, out_channels):\n",
        "        super().__init__(in_channels, out_channels, kernel_size=3, padding='same', bias=False)\n",
        "\n",
        "    def reset_parameters(self):\n",
        "        super().reset_parameters()\n",
        "        w = self.weight.data\n",
        "        torch.nn.init.dirac_(w[:w.size(1)])\n",
        "\n",
        "class ConvGroup(nn.Module):\n",
        "    def __init__(self, channels_in, channels_out):\n",
        "        super().__init__()\n",
        "        self.conv1 = Conv(channels_in,  channels_out)\n",
        "        self.pool = nn.MaxPool2d(2)\n",
        "        self.norm1 = BatchNorm(channels_out)\n",
        "        self.conv2 = Conv(channels_out, channels_out)\n",
        "        self.norm2 = BatchNorm(channels_out)\n",
        "        self.activ = nn.GELU()\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.conv1(x)\n",
        "        x = self.pool(x)\n",
        "        x = self.norm1(x)\n",
        "        x = self.activ(x)\n",
        "        x = self.conv2(x)\n",
        "        x = self.norm2(x)\n",
        "        x = self.activ(x)\n",
        "        return x\n",
        "\n",
        "class CifarNet(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        widths = dict(block1=64, block2=256, block3=256)\n",
        "        whiten_kernel_size = 2\n",
        "        whiten_width = 2 * 3 * whiten_kernel_size**2\n",
        "        self.whiten = nn.Conv2d(3, whiten_width, whiten_kernel_size, padding=0, bias=True)\n",
        "        self.whiten.weight.requires_grad = False\n",
        "        self.layers = nn.Sequential(\n",
        "            nn.GELU(),\n",
        "            ConvGroup(whiten_width,     widths['block1']),\n",
        "            ConvGroup(widths['block1'], widths['block2']),\n",
        "            ConvGroup(widths['block2'], widths['block3']),\n",
        "            nn.MaxPool2d(3),\n",
        "        )\n",
        "        self.head = nn.Linear(widths['block3'], 10, bias=False)\n",
        "        for mod in self.modules():\n",
        "            if isinstance(mod, BatchNorm):\n",
        "                mod.float()\n",
        "            else:\n",
        "                mod.half()\n",
        "\n",
        "    def reset(self):\n",
        "        for m in model.modules():\n",
        "            if type(m) in (nn.Conv2d, Conv, BatchNorm, nn.Linear):\n",
        "                m.reset_parameters()\n",
        "        w = self.head.weight.data\n",
        "        w *= 1 / w.std()\n",
        "\n",
        "    def init_whiten(self, train_images, eps=5e-4):\n",
        "        c, (h, w) = train_images.shape[1], self.whiten.weight.shape[2:]\n",
        "        patches = train_images.unfold(2,h,1).unfold(3,w,1).transpose(1,3).reshape(-1,c,h,w).float()\n",
        "        patches_flat = patches.view(len(patches), -1)\n",
        "        est_patch_covariance = (patches_flat.T @ patches_flat) / len(patches_flat)\n",
        "        eigenvalues, eigenvectors = torch.linalg.eigh(est_patch_covariance, UPLO='U')\n",
        "        eigenvectors_scaled = eigenvectors.T.reshape(-1,c,h,w) / torch.sqrt(eigenvalues.view(-1,1,1,1) + eps)\n",
        "        self.whiten.weight.data[:] = torch.cat((eigenvectors_scaled, -eigenvectors_scaled))\n",
        "\n",
        "    def forward(self, x, whiten_bias_grad=True):\n",
        "        b = self.whiten.bias\n",
        "        x = F.conv2d(x, self.whiten.weight, b if whiten_bias_grad else b.detach())\n",
        "        x = self.layers(x)\n",
        "        x = x.view(len(x), -1)\n",
        "        return self.head(x) / x.size(-1)\n",
        "\n",
        "############################################\n",
        "#                 Logging                  #\n",
        "############################################\n",
        "\n",
        "def print_columns(columns_list, is_head=False, is_final_entry=False):\n",
        "    print_string = ''\n",
        "    for col in columns_list:\n",
        "        print_string += '|  %s  ' % col\n",
        "    print_string += '|'\n",
        "    if is_head:\n",
        "        print('-'*len(print_string))\n",
        "    print(print_string)\n",
        "    if is_head or is_final_entry:\n",
        "        print('-'*len(print_string))\n",
        "\n",
        "logging_columns_list = ['run   ', 'epoch', 'train_acc', 'val_acc', 'tta_val_acc', 'time_seconds']\n",
        "def print_training_details(variables, is_final_entry):\n",
        "    formatted = []\n",
        "    for col in logging_columns_list:\n",
        "        var = variables.get(col.strip(), None)\n",
        "        if type(var) in (int, str):\n",
        "            res = str(var)\n",
        "        elif type(var) is float:\n",
        "            res = '{:0.4f}'.format(var)\n",
        "        else:\n",
        "            assert var is None\n",
        "            res = ''\n",
        "        formatted.append(res.rjust(len(col)))\n",
        "    print_columns(formatted, is_final_entry=is_final_entry)\n",
        "\n",
        "############################################\n",
        "#               Evaluation                 #\n",
        "############################################\n",
        "\n",
        "def infer(model, loader, tta_level=0):\n",
        "\n",
        "    # Test-time augmentation strategy (for tta_level=2):\n",
        "    # 1. Flip/mirror the image left-to-right (50% of the time).\n",
        "    # 2. Translate the image by one pixel either up-and-left or down-and-right (50% of the time,\n",
        "    #    i.e. both happen 25% of the time).\n",
        "    #\n",
        "    # This creates 6 views per image (left/right times the two translations and no-translation),\n",
        "    # which we evaluate and then weight according to the given probabilities.\n",
        "\n",
        "    def infer_basic(inputs, net):\n",
        "        return net(inputs).clone()\n",
        "\n",
        "    def infer_mirror(inputs, net):\n",
        "        return 0.5 * net(inputs) + 0.5 * net(inputs.flip(-1))\n",
        "\n",
        "    def infer_mirror_translate(inputs, net):\n",
        "        logits = infer_mirror(inputs, net)\n",
        "        pad = 1\n",
        "        padded_inputs = F.pad(inputs, (pad,)*4, 'reflect')\n",
        "        inputs_translate_list = [\n",
        "            padded_inputs[:, :, 0:32, 0:32],\n",
        "            padded_inputs[:, :, 2:34, 2:34],\n",
        "        ]\n",
        "        logits_translate_list = [infer_mirror(inputs_translate, net)\n",
        "                                 for inputs_translate in inputs_translate_list]\n",
        "        logits_translate = torch.stack(logits_translate_list).mean(0)\n",
        "        return 0.5 * logits + 0.5 * logits_translate\n",
        "\n",
        "    model.eval()\n",
        "    test_images = loader.normalize(loader.images)\n",
        "    infer_fn = [infer_basic, infer_mirror, infer_mirror_translate][tta_level]\n",
        "    with torch.no_grad():\n",
        "        return torch.cat([infer_fn(inputs, model) for inputs in test_images.split(2000)])\n",
        "\n",
        "def evaluate(model, loader, tta_level=0):\n",
        "    logits = infer(model, loader, tta_level)\n",
        "    return (logits.argmax(1) == loader.labels).float().mean().item()\n",
        "\n",
        "############################################\n",
        "#                Training                  #\n",
        "############################################\n",
        "\n",
        "def main(run, model):\n",
        "\n",
        "    batch_size = 2000\n",
        "    bias_lr = 0.053\n",
        "    head_lr = 0.67\n",
        "    wd = 2e-6 * batch_size\n",
        "\n",
        "    test_loader = CifarLoader('cifar10', train=False, batch_size=512)\n",
        "    train_loader = CifarLoader('cifar10', train=True, batch_size=batch_size, aug=dict(flip=True, translate=2))\n",
        "    if run == 'warmup':\n",
        "        # The only purpose of the first run is to warmup the compiled model, so we can use dummy data\n",
        "        train_loader.labels = torch.randint(0, 10, size=(len(train_loader.labels),), device=train_loader.labels.device)\n",
        "    total_train_steps = ceil(8 * len(train_loader))\n",
        "    whiten_bias_train_steps = ceil(3 * len(train_loader))\n",
        "\n",
        "    # Create optimizers and learning rate schedulers\n",
        "    filter_params = [p for p in model.parameters() if len(p.shape) == 4 and p.requires_grad]\n",
        "    norm_biases = [p for n, p in model.named_parameters() if 'norm' in n and p.requires_grad]\n",
        "    param_configs = [dict(params=[model.whiten.bias], lr=bias_lr, weight_decay=wd/bias_lr),\n",
        "                     dict(params=norm_biases, lr=bias_lr, weight_decay=wd/bias_lr),\n",
        "                     dict(params=[model.head.weight], lr=head_lr, weight_decay=wd/head_lr)]\n",
        "    optimizer1 = torch.optim.SGD(param_configs, momentum=0.85, nesterov=True, fused=True)\n",
        "    # optimizer2 = Muon(filter_params, lr=0.24, momentum=0.6, nesterov=True)\n",
        "    optimizer2 = SUMO(filter_params)\n",
        "    optimizers = [optimizer1, optimizer2]\n",
        "\n",
        "    for group in optimizer1.param_groups:\n",
        "      group['initial_lr'] = group['lr']\n",
        "\n",
        "    for group in optimizer2.param_groups:\n",
        "      group['initial_lr'] = group['lr']\n",
        "\n",
        "    # For accurately timing GPU code\n",
        "    starter = torch.cuda.Event(enable_timing=True)\n",
        "    ender = torch.cuda.Event(enable_timing=True)\n",
        "    time_seconds = 0.0\n",
        "    def start_timer():\n",
        "        starter.record()\n",
        "    def stop_timer():\n",
        "        ender.record()\n",
        "        torch.cuda.synchronize()\n",
        "        nonlocal time_seconds\n",
        "        time_seconds += 1e-3 * starter.elapsed_time(ender)\n",
        "\n",
        "    model.reset()\n",
        "    step = 0\n",
        "\n",
        "    # Initialize the whitening layer using training images\n",
        "    # wandb.init()\n",
        "    start_timer()\n",
        "    train_images = train_loader.normalize(train_loader.images[:5000])\n",
        "    model.init_whiten(train_images)\n",
        "    stop_timer()\n",
        "\n",
        "    for epoch in range(ceil(total_train_steps / len(train_loader))):\n",
        "\n",
        "        ####################\n",
        "        #     Training     #\n",
        "        ####################\n",
        "\n",
        "        start_timer()\n",
        "        model.train()\n",
        "        for inputs, labels in train_loader:\n",
        "            outputs = model(inputs, whiten_bias_grad=(step < whiten_bias_train_steps))\n",
        "            F.cross_entropy(outputs, labels, label_smoothing=0.2, reduction='sum').backward()\n",
        "            for group in optimizer1.param_groups[:1]:\n",
        "                group[\"lr\"] = group[\"initial_lr\"] * (1 - step / whiten_bias_train_steps)\n",
        "            for group in optimizer1.param_groups[1:]+optimizer2.param_groups:\n",
        "                group[\"lr\"] = group[\"initial_lr\"] * (1 - step / total_train_steps)\n",
        "            for opt in optimizers:\n",
        "                opt.step()\n",
        "            model.zero_grad(set_to_none=True)\n",
        "            step += 1\n",
        "            if step >= total_train_steps:\n",
        "                break\n",
        "        stop_timer()\n",
        "        torch.cuda.synchronize()\n",
        "        torch.cuda.empty_cache()\n",
        "        ####################\n",
        "        #    Evaluation    #\n",
        "        ####################\n",
        "\n",
        "        # Save the accuracy and loss from the last training batch of the epoch\n",
        "        train_acc = (outputs.detach().argmax(1) == labels).float().mean().item()\n",
        "        val_acc = evaluate(model, test_loader, tta_level=0)\n",
        "        print_training_details(locals(), is_final_entry=False)\n",
        "        run = None # Only print the run number once\n",
        "\n",
        "\n",
        "    ####################\n",
        "    #  TTA Evaluation  #\n",
        "    ####################\n",
        "    # wandb.finish()\n",
        "    start_timer()\n",
        "    tta_val_acc = evaluate(model, test_loader, tta_level=0)\n",
        "    stop_timer()\n",
        "    epoch = 'eval'\n",
        "    print_training_details(locals(), is_final_entry=True)\n",
        "\n",
        "    return tta_val_acc"
      ],
      "metadata": {
        "id": "VTLi2HGrwT65"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "jMuVgzWCuoc6"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if __name__ == \"__main__\":\n",
        "\n",
        "    # We re-use the compiled model between runs to save the non-data-dependent compilation time\n",
        "    model = CifarNet().cuda().to(memory_format=torch.channels_last)\n",
        "    model.compile(mode='max-autotune')\n",
        "\n",
        "    print_columns(logging_columns_list, is_head=True)\n",
        "    main('warmup', model)\n",
        "    accs = torch.tensor([main(run, model) for run in range(200)])\n",
        "    print('Mean: %.4f    Std: %.4f' % (accs.mean(), accs.std()))\n",
        "\n",
        "    log_dir = os.path.join('logs', str(uuid.uuid4()))\n",
        "    os.makedirs(log_dir, exist_ok=True)\n",
        "    log_path = os.path.join(log_dir, 'log.pt')\n",
        "    torch.save(dict(code=code, accs=accs), log_path)\n",
        "    print(os.path.abspath(log_path))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "g5A0pAszxvQP",
        "outputId": "0da2ddb9-947c-4342-8012-f6f3416abb8e"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "---------------------------------------------------------------------------------\n",
            "|  run     |  epoch  |  train_acc  |  val_acc  |  tta_val_acc  |  time_seconds  |\n",
            "---------------------------------------------------------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 170M/170M [00:04<00:00, 42.5MB/s]\n",
            "/usr/local/lib/python3.12/dist-packages/torch/backends/cuda/__init__.py:131: UserWarning: Please use the new API settings to control TF32 behavior, such as torch.backends.cudnn.conv.fp32_precision = 'tf32' or torch.backends.cuda.matmul.fp32_precision = 'ieee'. Old settings, e.g, torch.backends.cuda.matmul.allow_tf32 = True, torch.backends.cudnn.allow_tf32 = True, allowTF32CuDNN() and allowTF32CuBLAS() will be deprecated after Pytorch 2.9. Please see https://pytorch.org/docs/main/notes/cuda.html#tensorfloat-32-tf32-on-ampere-and-later-devices (Triggered internally at /pytorch/aten/src/ATen/Context.cpp:80.)\n",
            "  return torch._C._get_cublas_allow_tf32()\n",
            "W1210 10:17:51.272000 192 torch/_inductor/utils.py:1558] [0/0] Not enough SMs to use max_autotune_gemm mode\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "|  warmup  |      0  |     0.0875  |   0.1113  |               |      116.4343  |\n",
            "|          |      1  |     0.1000  |   0.1019  |               |      126.2897  |\n",
            "|          |      2  |     0.0925  |   0.0857  |               |      136.3367  |\n",
            "|          |      3  |     0.1120  |   0.1167  |               |      183.8164  |\n",
            "|          |      4  |     0.1250  |   0.1072  |               |      194.1475  |\n",
            "|          |      5  |     0.1115  |   0.1073  |               |      204.6166  |\n",
            "|          |      6  |     0.1195  |   0.1082  |               |      215.1058  |\n",
            "|          |      7  |     0.1310  |   0.0938  |               |      225.4990  |\n",
            "|          |   eval  |     0.1310  |   0.0938  |       0.0938  |      225.6839  |\n",
            "---------------------------------------------------------------------------------\n",
            "|       0  |      0  |     0.5090  |   0.3644  |               |       10.4919  |\n",
            "|          |      1  |     0.6485  |   0.6299  |               |       20.8188  |\n",
            "|          |      2  |     0.7255  |   0.7166  |               |       31.1584  |\n",
            "|          |      3  |     0.7565  |   0.7548  |               |       41.4222  |\n",
            "|          |      4  |     0.7860  |   0.7751  |               |       51.7250  |\n",
            "|          |      5  |     0.8160  |   0.7941  |               |       62.0551  |\n",
            "|          |      6  |     0.8055  |   0.7953  |               |       72.4048  |\n",
            "|          |      7  |     0.8350  |   0.8060  |               |       82.7432  |\n",
            "|          |   eval  |     0.8350  |   0.8060  |       0.8060  |       82.9281  |\n",
            "---------------------------------------------------------------------------------\n",
            "|       1  |      0  |     0.5185  |   0.3996  |               |       10.5455  |\n",
            "|          |      1  |     0.6230  |   0.6556  |               |       20.9711  |\n",
            "|          |      2  |     0.6990  |   0.7106  |               |       31.4204  |\n",
            "|          |      3  |     0.7640  |   0.7384  |               |       41.7222  |\n",
            "|          |      4  |     0.7870  |   0.7774  |               |       52.0207  |\n",
            "|          |      5  |     0.7955  |   0.7932  |               |       62.3136  |\n",
            "|          |      6  |     0.8305  |   0.8030  |               |       72.6126  |\n",
            "|          |      7  |     0.8190  |   0.8025  |               |       82.9243  |\n",
            "|          |   eval  |     0.8190  |   0.8025  |       0.8025  |       83.1073  |\n",
            "---------------------------------------------------------------------------------\n",
            "|       2  |      0  |     0.5090  |   0.4747  |               |       10.5528  |\n",
            "|          |      1  |     0.6455  |   0.5739  |               |       20.9898  |\n",
            "|          |      2  |     0.7325  |   0.6949  |               |       31.4558  |\n",
            "|          |      3  |     0.7550  |   0.7412  |               |       41.7747  |\n",
            "|          |      4  |     0.7780  |   0.7675  |               |       52.0764  |\n",
            "|          |      5  |     0.7905  |   0.7868  |               |       62.3947  |\n",
            "|          |      6  |     0.8155  |   0.7948  |               |       72.6984  |\n",
            "|          |      7  |     0.8105  |   0.7988  |               |       83.0323  |\n",
            "|          |   eval  |     0.8105  |   0.7988  |       0.7988  |       83.2115  |\n",
            "---------------------------------------------------------------------------------\n",
            "|       3  |      0  |     0.4650  |   0.3551  |               |       10.5307  |\n",
            "|          |      1  |     0.6285  |   0.6460  |               |       20.9573  |\n",
            "|          |      2  |     0.7145  |   0.7091  |               |       31.4129  |\n",
            "|          |      3  |     0.7460  |   0.7619  |               |       41.7339  |\n",
            "|          |      4  |     0.7905  |   0.7779  |               |       52.0536  |\n",
            "|          |      5  |     0.8065  |   0.7943  |               |       62.3939  |\n",
            "|          |      6  |     0.8225  |   0.8052  |               |       72.7343  |\n",
            "|          |      7  |     0.8200  |   0.8063  |               |       83.0450  |\n",
            "|          |   eval  |     0.8200  |   0.8063  |       0.8063  |       83.2313  |\n",
            "---------------------------------------------------------------------------------\n",
            "|       4  |      0  |     0.5185  |   0.3904  |               |       10.4767  |\n",
            "|          |      1  |     0.6360  |   0.6292  |               |       20.8565  |\n",
            "|          |      2  |     0.7015  |   0.7224  |               |       31.2759  |\n",
            "|          |      3  |     0.7595  |   0.7591  |               |       41.5863  |\n",
            "|          |      4  |     0.7920  |   0.7738  |               |       51.9069  |\n",
            "|          |      5  |     0.8085  |   0.7951  |               |       62.2075  |\n",
            "|          |      6  |     0.8110  |   0.8010  |               |       72.5085  |\n",
            "|          |      7  |     0.8180  |   0.8100  |               |       82.8114  |\n",
            "|          |   eval  |     0.8180  |   0.8100  |       0.8100  |       82.9863  |\n",
            "---------------------------------------------------------------------------------\n",
            "|       5  |      0  |     0.5115  |   0.4223  |               |       10.5177  |\n",
            "|          |      1  |     0.6615  |   0.6595  |               |       20.9451  |\n",
            "|          |      2  |     0.7240  |   0.7239  |               |       31.4203  |\n",
            "|          |      3  |     0.7880  |   0.7464  |               |       41.7379  |\n",
            "|          |      4  |     0.7945  |   0.7576  |               |       52.0442  |\n",
            "|          |      5  |     0.7945  |   0.7945  |               |       62.3369  |\n",
            "|          |      6  |     0.8450  |   0.8081  |               |       72.6236  |\n",
            "|          |      7  |     0.8335  |   0.8111  |               |       82.9587  |\n",
            "|          |   eval  |     0.8335  |   0.8111  |       0.8111  |       83.1437  |\n",
            "---------------------------------------------------------------------------------\n",
            "|       6  |      0  |     0.4930  |   0.3269  |               |       10.4978  |\n",
            "|          |      1  |     0.6430  |   0.5953  |               |       20.9081  |\n",
            "|          |      2  |     0.7125  |   0.7194  |               |       31.3527  |\n",
            "|          |      3  |     0.7555  |   0.7605  |               |       41.7236  |\n",
            "|          |      4  |     0.7885  |   0.7836  |               |       52.0188  |\n",
            "|          |      5  |     0.7975  |   0.7912  |               |       62.3460  |\n",
            "|          |      6  |     0.8205  |   0.8026  |               |       72.6567  |\n",
            "|          |      7  |     0.8200  |   0.8101  |               |       83.0119  |\n",
            "|          |   eval  |     0.8200  |   0.8101  |       0.8101  |       83.1945  |\n",
            "---------------------------------------------------------------------------------\n",
            "|       7  |      0  |     0.4800  |   0.4031  |               |       10.4855  |\n",
            "|          |      1  |     0.6250  |   0.6259  |               |       20.8772  |\n",
            "|          |      2  |     0.7135  |   0.7216  |               |       31.3175  |\n",
            "|          |      3  |     0.7770  |   0.7473  |               |       41.6460  |\n",
            "|          |      4  |     0.7980  |   0.7631  |               |       51.9681  |\n",
            "|          |      5  |     0.8045  |   0.7870  |               |       62.3111  |\n",
            "|          |      6  |     0.8105  |   0.8029  |               |       72.6445  |\n",
            "|          |      7  |     0.8370  |   0.8074  |               |       82.9584  |\n",
            "|          |   eval  |     0.8370  |   0.8074  |       0.8074  |       83.1422  |\n",
            "---------------------------------------------------------------------------------\n",
            "|       8  |      0  |     0.4970  |   0.4258  |               |       10.5227  |\n",
            "|          |      1  |     0.6585  |   0.5734  |               |       20.9357  |\n",
            "|          |      2  |     0.7125  |   0.6614  |               |       31.3893  |\n",
            "|          |      3  |     0.7610  |   0.6660  |               |       41.7089  |\n",
            "|          |      4  |     0.7765  |   0.7796  |               |       52.0321  |\n",
            "|          |      5  |     0.8005  |   0.7957  |               |       62.3338  |\n",
            "|          |      6  |     0.8195  |   0.7991  |               |       72.6151  |\n",
            "|          |      7  |     0.8255  |   0.8074  |               |       82.9137  |\n",
            "|          |   eval  |     0.8255  |   0.8074  |       0.8074  |       83.0909  |\n",
            "---------------------------------------------------------------------------------\n",
            "|       9  |      0  |     0.5020  |   0.3922  |               |       10.4689  |\n",
            "|          |      1  |     0.6455  |   0.6524  |               |       20.8595  |\n",
            "|          |      2  |     0.7020  |   0.7128  |               |       31.3009  |\n",
            "|          |      3  |     0.7505  |   0.7437  |               |       41.6287  |\n",
            "|          |      4  |     0.7855  |   0.7742  |               |       51.9546  |\n",
            "|          |      5  |     0.8015  |   0.7891  |               |       62.2972  |\n",
            "|          |      6  |     0.8055  |   0.8019  |               |       72.6128  |\n",
            "|          |      7  |     0.8110  |   0.8060  |               |       82.9416  |\n",
            "|          |   eval  |     0.8110  |   0.8060  |       0.8060  |       83.1209  |\n",
            "---------------------------------------------------------------------------------\n",
            "|      10  |      0  |     0.5405  |   0.4536  |               |       10.5340  |\n",
            "|          |      1  |     0.6400  |   0.6742  |               |       20.9423  |\n",
            "|          |      2  |     0.7250  |   0.7289  |               |       31.3679  |\n",
            "|          |      3  |     0.7625  |   0.7634  |               |       41.6503  |\n",
            "|          |      4  |     0.7870  |   0.7867  |               |       51.9563  |\n",
            "|          |      5  |     0.8240  |   0.7904  |               |       62.2531  |\n",
            "|          |      6  |     0.8255  |   0.8080  |               |       72.5670  |\n",
            "|          |      7  |     0.8215  |   0.8098  |               |       82.9023  |\n",
            "|          |   eval  |     0.8215  |   0.8098  |       0.8098  |       83.0836  |\n",
            "---------------------------------------------------------------------------------\n",
            "|      11  |      0  |     0.5155  |   0.4555  |               |       10.5517  |\n",
            "|          |      1  |     0.6255  |   0.6382  |               |       20.9885  |\n",
            "|          |      2  |     0.7285  |   0.7051  |               |       31.4479  |\n",
            "|          |      3  |     0.7800  |   0.7455  |               |       41.7689  |\n",
            "|          |      4  |     0.8015  |   0.7689  |               |       52.0763  |\n",
            "|          |      5  |     0.7935  |   0.7978  |               |       62.3995  |\n",
            "|          |      6  |     0.8230  |   0.8044  |               |       72.7092  |\n",
            "|          |      7  |     0.8110  |   0.8085  |               |       82.9965  |\n",
            "|          |   eval  |     0.8110  |   0.8085  |       0.8085  |       83.1734  |\n",
            "---------------------------------------------------------------------------------\n",
            "|      12  |      0  |     0.5160  |   0.4881  |               |       10.4751  |\n",
            "|          |      1  |     0.6515  |   0.6468  |               |       20.8692  |\n",
            "|          |      2  |     0.7435  |   0.7068  |               |       31.3152  |\n",
            "|          |      3  |     0.7560  |   0.7558  |               |       41.6379  |\n",
            "|          |      4  |     0.7890  |   0.7658  |               |       51.9734  |\n",
            "|          |      5  |     0.8090  |   0.7946  |               |       62.3016  |\n",
            "|          |      6  |     0.8070  |   0.8066  |               |       72.6408  |\n",
            "|          |      7  |     0.8215  |   0.8075  |               |       82.9659  |\n",
            "|          |   eval  |     0.8215  |   0.8075  |       0.8075  |       83.1471  |\n",
            "---------------------------------------------------------------------------------\n",
            "|      13  |      0  |     0.5290  |   0.4580  |               |       10.5160  |\n",
            "|          |      1  |     0.6770  |   0.6232  |               |       20.9414  |\n",
            "|          |      2  |     0.7265  |   0.7091  |               |       31.3797  |\n",
            "|          |      3  |     0.7630  |   0.7603  |               |       41.6854  |\n",
            "|          |      4  |     0.7860  |   0.7854  |               |       51.9944  |\n",
            "|          |      5  |     0.8070  |   0.7860  |               |       62.3206  |\n",
            "|          |      6  |     0.8355  |   0.7993  |               |       72.6529  |\n",
            "|          |      7  |     0.8295  |   0.8053  |               |       82.9969  |\n",
            "|          |   eval  |     0.8295  |   0.8053  |       0.8053  |       83.1911  |\n",
            "---------------------------------------------------------------------------------\n",
            "|      14  |      0  |     0.4830  |   0.4803  |               |       10.5317  |\n",
            "|          |      1  |     0.6400  |   0.6072  |               |       20.9325  |\n",
            "|          |      2  |     0.7135  |   0.7161  |               |       31.3695  |\n",
            "|          |      3  |     0.7460  |   0.7546  |               |       41.6619  |\n",
            "|          |      4  |     0.7885  |   0.7750  |               |       51.9524  |\n",
            "|          |      5  |     0.8080  |   0.7868  |               |       62.2571  |\n",
            "|          |      6  |     0.8165  |   0.8004  |               |       72.5926  |\n",
            "|          |      7  |     0.8300  |   0.8021  |               |       82.9372  |\n",
            "|          |   eval  |     0.8300  |   0.8021  |       0.8021  |       83.1212  |\n",
            "---------------------------------------------------------------------------------\n",
            "|      15  |      0  |     0.5110  |   0.4607  |               |       10.5380  |\n",
            "|          |      1  |     0.6715  |   0.6670  |               |       20.9665  |\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-1080971013.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0mprint_columns\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogging_columns_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mis_head\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'warmup'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m     \u001b[0maccs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mrun\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m200\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Mean: %.4f    Std: %.4f'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0maccs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maccs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-2592139650.py\u001b[0m in \u001b[0;36mmain\u001b[0;34m(run, model)\u001b[0m\n\u001b[1;32m    442\u001b[0m                 \u001b[0mgroup\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"lr\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgroup\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"initial_lr\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mstep\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mtotal_train_steps\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    443\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mopt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0moptimizers\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 444\u001b[0;31m                 \u001b[0mopt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    445\u001b[0m             \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mset_to_none\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    446\u001b[0m             \u001b[0mstep\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/optim/optimizer.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    515\u001b[0m                             )\n\u001b[1;32m    516\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 517\u001b[0;31m                 \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    518\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_optimizer_step_code\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    519\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-2592139650.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     95\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     96\u001b[0m         \u001b[0;31m# O_float is computed from M_float, so O_float is float\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 97\u001b[0;31m         \u001b[0mO_float\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0morthogonalization_svd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mM_float\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# (r, n) if r <= k else (k, n)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     98\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     99\u001b[0m         \u001b[0;31m# prevent zeroing out o_norm initially\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/_dynamo/eval_frame.py\u001b[0m in \u001b[0;36mcompile_wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    830\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    831\u001b[0m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 832\u001b[0;31m                     \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    833\u001b[0m                 \u001b[0;32mexcept\u001b[0m \u001b[0mUnsupported\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    834\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mverbose\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-2592139650.py\u001b[0m in \u001b[0;36morthogonalization_svd\u001b[0;34m(M)\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;31m#############################################\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m \u001b[0;34m@\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompile\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0morthogonalization_svd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mM\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m   \u001b[0mU\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0msig\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mV\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinalg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msvd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mM\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfull_matrices\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# TODO: figuring out full_matrices=True case\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/_dynamo/eval_frame.py\u001b[0m in \u001b[0;36m_fn\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m   1042\u001b[0m                 \u001b[0m_maybe_set_eval_frame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_callback_from_stance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcallback\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1043\u001b[0m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1044\u001b[0;31m                     \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1045\u001b[0m                 \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1046\u001b[0m                     \u001b[0mset_eval_frame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/_functorch/aot_autograd.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(*runtime_args)\u001b[0m\n\u001b[1;32m   1128\u001b[0m             \u001b[0mfull_args\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparams_buffers_flat\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1129\u001b[0m             \u001b[0mfull_args\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mruntime_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1130\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mcompiled_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfull_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1131\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1132\u001b[0m     \u001b[0;31m# Just for convenience\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/_functorch/_aot_autograd/runtime_wrappers.py\u001b[0m in \u001b[0;36mruntime_wrapper\u001b[0;34m(args)\u001b[0m\n\u001b[1;32m    351\u001b[0m                     \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_set_grad_enabled\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    352\u001b[0m                 \u001b[0mrecord_runtime_wrapper_prologue_exit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcm\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 353\u001b[0;31m                 all_outs = call_func_at_runtime_with_args(\n\u001b[0m\u001b[1;32m    354\u001b[0m                     \u001b[0mcompiled_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdisable_amp\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdisable_amp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msteal_args\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    355\u001b[0m                 )\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/_functorch/_aot_autograd/utils.py\u001b[0m in \u001b[0;36mcall_func_at_runtime_with_args\u001b[0;34m(f, args, steal_args, disable_amp)\u001b[0m\n\u001b[1;32m    127\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mcontext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    128\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"_boxed_call\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 129\u001b[0;31m             \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnormalize_as_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    130\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    131\u001b[0m             \u001b[0;31m# TODO: Please remove soon\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/_functorch/_aot_autograd/runtime_wrappers.py\u001b[0m in \u001b[0;36minner_fn\u001b[0;34m(args)\u001b[0m\n\u001b[1;32m    722\u001b[0m                 \u001b[0mold_args\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    723\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 724\u001b[0;31m             \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompiled_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    725\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    726\u001b[0m             \u001b[0;31m# Inductor cache DummyModule can return None\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/_functorch/_aot_autograd/runtime_wrappers.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(runtime_args)\u001b[0m\n\u001b[1;32m    524\u001b[0m                 )\n\u001b[1;32m    525\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 526\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mcompiled_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mruntime_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    527\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    528\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/_inductor/output_code.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m    611\u001b[0m                 \u001b[0;34mf\"## Call CompiledFxGraph {self._fx_graph_cache_key} ##\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    612\u001b[0m             ):\n\u001b[0;32m--> 613\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcurrent_callable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    614\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    615\u001b[0m             \u001b[0mget_runtime_metrics_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfinish\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/_inductor/utils.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(new_inputs)\u001b[0m\n\u001b[1;32m   2960\u001b[0m             \u001b[0mnew_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs_to_check\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmutated_input_idxs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2961\u001b[0m         )\n\u001b[0;32m-> 2962\u001b[0;31m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnew_inputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2963\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2964\u001b[0m         \u001b[0;31m# If a mutated tensor was cloned to be aligned, we need to reflect back the mutation to the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/torchinductor_root/jg/cjg3hud4r6aqethbq25eifh2vant5eyp5qxb736dsqb72x6uy2xu.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, args)\u001b[0m\n\u001b[1;32m    147\u001b[0m             \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_device\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    148\u001b[0m             \u001b[0;31m# Topologically Sorted Source Nodes: [linalg_svd], Original ATen: [aten._linalg_svd]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 149\u001b[0;31m             \u001b[0mbuf0\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maten\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_linalg_svd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdefault\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marg2_1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    150\u001b[0m             \u001b[0;32mdel\u001b[0m \u001b[0marg2_1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    151\u001b[0m             \u001b[0mbuf1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbuf0\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/_ops.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    839\u001b[0m     \u001b[0;31m# that are named \"self\". This way, all the aten ops can be called by kwargs.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    840\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m/\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0m_P\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0m_P\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0m_T\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 841\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_op\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    842\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    843\u001b[0m     \u001b[0;31m# Use positional-only argument to avoid naming collision with aten ops arguments\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "torch.cuda.memory._dump_snapshot(\"/content/sumo_expandable_segments_snapshot.pickle\")"
      ],
      "metadata": {
        "id": "a3j3EIwCyeG7"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "torch.cuda.synchronize()\n",
        "stats = torch.cuda.memory_stats()"
      ],
      "metadata": {
        "id": "wUb727SDy8qv"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "stats"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TxoItWHr5VeB",
        "outputId": "3f39f658-0b1f-432c-d8dc-d9a82b20919f"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "OrderedDict([('active.all.allocated', 691114),\n",
              "             ('active.all.current', 85),\n",
              "             ('active.all.freed', 691029),\n",
              "             ('active.all.peak', 127),\n",
              "             ('active.large_pool.allocated', 176118),\n",
              "             ('active.large_pool.current', 20),\n",
              "             ('active.large_pool.freed', 176098),\n",
              "             ('active.large_pool.peak', 46),\n",
              "             ('active.small_pool.allocated', 514996),\n",
              "             ('active.small_pool.current', 65),\n",
              "             ('active.small_pool.freed', 514931),\n",
              "             ('active.small_pool.peak', 85),\n",
              "             ('active_bytes.all.allocated', 771592927744),\n",
              "             ('active_bytes.all.current', 1457926144),\n",
              "             ('active_bytes.all.freed', 770135001600),\n",
              "             ('active_bytes.all.peak', 9683783168),\n",
              "             ('active_bytes.large_pool.allocated', 718051320832),\n",
              "             ('active_bytes.large_pool.current', 1453645312),\n",
              "             ('active_bytes.large_pool.freed', 716597675520),\n",
              "             ('active_bytes.large_pool.peak', 9679396352),\n",
              "             ('active_bytes.small_pool.allocated', 53541606912),\n",
              "             ('active_bytes.small_pool.current', 4280832),\n",
              "             ('active_bytes.small_pool.freed', 53537326080),\n",
              "             ('active_bytes.small_pool.peak', 9421824),\n",
              "             ('allocated_bytes.all.allocated', 771592927744),\n",
              "             ('allocated_bytes.all.current', 1457926144),\n",
              "             ('allocated_bytes.all.freed', 770135001600),\n",
              "             ('allocated_bytes.all.peak', 9683783168),\n",
              "             ('allocated_bytes.large_pool.allocated', 718051320832),\n",
              "             ('allocated_bytes.large_pool.current', 1453645312),\n",
              "             ('allocated_bytes.large_pool.freed', 716597675520),\n",
              "             ('allocated_bytes.large_pool.peak', 9679396352),\n",
              "             ('allocated_bytes.small_pool.allocated', 53541606912),\n",
              "             ('allocated_bytes.small_pool.current', 4280832),\n",
              "             ('allocated_bytes.small_pool.freed', 53537326080),\n",
              "             ('allocated_bytes.small_pool.peak', 9421824),\n",
              "             ('allocation.all.allocated', 691114),\n",
              "             ('allocation.all.current', 85),\n",
              "             ('allocation.all.freed', 691029),\n",
              "             ('allocation.all.peak', 127),\n",
              "             ('allocation.large_pool.allocated', 176118),\n",
              "             ('allocation.large_pool.current', 20),\n",
              "             ('allocation.large_pool.freed', 176098),\n",
              "             ('allocation.large_pool.peak', 46),\n",
              "             ('allocation.small_pool.allocated', 514996),\n",
              "             ('allocation.small_pool.current', 65),\n",
              "             ('allocation.small_pool.freed', 514931),\n",
              "             ('allocation.small_pool.peak', 85),\n",
              "             ('inactive_split.all.allocated', 0),\n",
              "             ('inactive_split.all.current', 0),\n",
              "             ('inactive_split.all.freed', 0),\n",
              "             ('inactive_split.all.peak', 0),\n",
              "             ('inactive_split.large_pool.allocated', 0),\n",
              "             ('inactive_split.large_pool.current', 0),\n",
              "             ('inactive_split.large_pool.freed', 0),\n",
              "             ('inactive_split.large_pool.peak', 0),\n",
              "             ('inactive_split.small_pool.allocated', 0),\n",
              "             ('inactive_split.small_pool.current', 0),\n",
              "             ('inactive_split.small_pool.freed', 0),\n",
              "             ('inactive_split.small_pool.peak', 0),\n",
              "             ('inactive_split_bytes.all.allocated', 0),\n",
              "             ('inactive_split_bytes.all.current', 0),\n",
              "             ('inactive_split_bytes.all.freed', 0),\n",
              "             ('inactive_split_bytes.all.peak', 0),\n",
              "             ('inactive_split_bytes.large_pool.allocated', 0),\n",
              "             ('inactive_split_bytes.large_pool.current', 0),\n",
              "             ('inactive_split_bytes.large_pool.freed', 0),\n",
              "             ('inactive_split_bytes.large_pool.peak', 0),\n",
              "             ('inactive_split_bytes.small_pool.allocated', 0),\n",
              "             ('inactive_split_bytes.small_pool.current', 0),\n",
              "             ('inactive_split_bytes.small_pool.freed', 0),\n",
              "             ('inactive_split_bytes.small_pool.peak', 0),\n",
              "             ('max_split_size', -1),\n",
              "             ('num_alloc_retries', 0),\n",
              "             ('num_device_alloc', 572),\n",
              "             ('num_device_free', 415),\n",
              "             ('num_ooms', 0),\n",
              "             ('num_sync_all_streams', 136),\n",
              "             ('oversize_allocations.allocated', 0),\n",
              "             ('oversize_allocations.current', 0),\n",
              "             ('oversize_allocations.freed', 0),\n",
              "             ('oversize_allocations.peak', 0),\n",
              "             ('oversize_segments.allocated', 0),\n",
              "             ('oversize_segments.current', 0),\n",
              "             ('oversize_segments.freed', 0),\n",
              "             ('oversize_segments.peak', 0),\n",
              "             ('requested_bytes.all.allocated', 771474044712),\n",
              "             ('requested_bytes.all.current', 1457916008),\n",
              "             ('requested_bytes.all.freed', 770016128704),\n",
              "             ('requested_bytes.all.peak', 9683773280),\n",
              "             ('requested_bytes.large_pool.allocated', 718051194441),\n",
              "             ('requested_bytes.large_pool.current', 1453645312),\n",
              "             ('requested_bytes.large_pool.freed', 716597549129),\n",
              "             ('requested_bytes.large_pool.peak', 9679396352),\n",
              "             ('requested_bytes.small_pool.allocated', 53422850271),\n",
              "             ('requested_bytes.small_pool.current', 4270696),\n",
              "             ('requested_bytes.small_pool.freed', 53418579575),\n",
              "             ('requested_bytes.small_pool.peak', 9409436),\n",
              "             ('reserved_bytes.all.allocated', 77460406272),\n",
              "             ('reserved_bytes.all.current', 9831448576),\n",
              "             ('reserved_bytes.all.freed', 67628957696),\n",
              "             ('reserved_bytes.all.peak', 10146021376),\n",
              "             ('reserved_bytes.large_pool.allocated', 76986449920),\n",
              "             ('reserved_bytes.large_pool.current', 9814671360),\n",
              "             ('reserved_bytes.large_pool.freed', 67171778560),\n",
              "             ('reserved_bytes.large_pool.peak', 10129244160),\n",
              "             ('reserved_bytes.small_pool.allocated', 473956352),\n",
              "             ('reserved_bytes.small_pool.current', 16777216),\n",
              "             ('reserved_bytes.small_pool.freed', 457179136),\n",
              "             ('reserved_bytes.small_pool.peak', 18874368),\n",
              "             ('segment.all.allocated', 0),\n",
              "             ('segment.all.current', 0),\n",
              "             ('segment.all.freed', 0),\n",
              "             ('segment.all.peak', 0),\n",
              "             ('segment.large_pool.allocated', 0),\n",
              "             ('segment.large_pool.current', 0),\n",
              "             ('segment.large_pool.freed', 0),\n",
              "             ('segment.large_pool.peak', 0),\n",
              "             ('segment.small_pool.allocated', 0),\n",
              "             ('segment.small_pool.current', 0),\n",
              "             ('segment.small_pool.freed', 0),\n",
              "             ('segment.small_pool.peak', 0)])"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        " print(f\"[{\"after code\"}] allocated={torch.cuda.memory_allocated()/1024**2:.1f}MB  \"\n",
        "          f\"reserved={torch.cuda.memory_reserved()/1024**2:.1f}MB  \"\n",
        "          f\"largest_block={stats['largest_block_bytes.all.current']/1024**2:.1f}MB  \"\n",
        "          f\"inactive_split={stats['inactive_split_bytes.all.current']/1024**2:.1f}MB\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 193
        },
        "id": "f0evQMrz5ZnS",
        "outputId": "b2594bbb-823e-4c3f-b3d1-1c005a82d739"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "error",
          "ename": "KeyError",
          "evalue": "'largest_block_bytes.all.current'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-3937746954.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m print(f\"[{\"after code\"}] allocated={torch.cuda.memory_allocated()/1024**2:.1f}MB  \"\n\u001b[1;32m      2\u001b[0m          \u001b[0;34mf\"reserved={torch.cuda.memory_reserved()/1024**2:.1f}MB  \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m          \u001b[0;34mf\"largest_block={stats['largest_block_bytes.all.current']/1024**2:.1f}MB  \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m          f\"inactive_split={stats['inactive_split_bytes.all.current']/1024**2:.1f}MB\")\n",
            "\u001b[0;31mKeyError\u001b[0m: 'largest_block_bytes.all.current'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "summary = torch.cuda.memory.memory_summary()"
      ],
      "metadata": {
        "id": "USzc1L8R9N5g"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(summary)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zO6JGScB_rAF",
        "outputId": "57da6f88-0ac3-4473-8e85-9c1ae08510f0"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "|===========================================================================|\n",
            "|                  PyTorch CUDA memory summary, device ID 0                 |\n",
            "|---------------------------------------------------------------------------|\n",
            "|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |\n",
            "|===========================================================================|\n",
            "|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocated memory      |   1390 MiB |   9235 MiB | 735848 MiB | 734457 MiB |\n",
            "|       from large pool |   1386 MiB |   9230 MiB | 684787 MiB | 683400 MiB |\n",
            "|       from small pool |      4 MiB |      8 MiB |  51061 MiB |  51057 MiB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active memory         |   1390 MiB |   9235 MiB | 735848 MiB | 734457 MiB |\n",
            "|       from large pool |   1386 MiB |   9230 MiB | 684787 MiB | 683400 MiB |\n",
            "|       from small pool |      4 MiB |      8 MiB |  51061 MiB |  51057 MiB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Requested memory      |   1390 MiB |   9235 MiB | 735734 MiB | 734344 MiB |\n",
            "|       from large pool |   1386 MiB |   9230 MiB | 684786 MiB | 683400 MiB |\n",
            "|       from small pool |      4 MiB |      8 MiB |  50948 MiB |  50943 MiB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved memory   |   9376 MiB |   9676 MiB |  73872 MiB |  64496 MiB |\n",
            "|       from large pool |   9360 MiB |   9660 MiB |  73420 MiB |  64060 MiB |\n",
            "|       from small pool |     16 MiB |     18 MiB |    452 MiB |    436 MiB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |\n",
            "|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |\n",
            "|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocations           |      85    |     127    |  691114    |  691029    |\n",
            "|       from large pool |      20    |      46    |  176118    |  176098    |\n",
            "|       from small pool |      65    |      85    |  514996    |  514931    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active allocs         |      85    |     127    |  691114    |  691029    |\n",
            "|       from large pool |      20    |      46    |  176118    |  176098    |\n",
            "|       from small pool |      65    |      85    |  514996    |  514931    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved segments |       0    |       0    |       0    |       0    |\n",
            "|       from large pool |       0    |       0    |       0    |       0    |\n",
            "|       from small pool |       0    |       0    |       0    |       0    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable allocs |       0    |       0    |       0    |       0    |\n",
            "|       from large pool |       0    |       0    |       0    |       0    |\n",
            "|       from small pool |       0    |       0    |       0    |       0    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Oversize allocations  |       0    |       0    |       0    |       0    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Oversize GPU segments |       0    |       0    |       0    |       0    |\n",
            "|===========================================================================|\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "hCQpMnXw_rjJ"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}